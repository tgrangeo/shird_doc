import { readFile, writeFile, mkdir, rm, stat } from 'node:fs/promises';
import { tryUseNuxt, resolveAlias, useLogger, isIgnored, updateTemplates, addVitePlugin, useNuxt, resolveModule, installModule, extendViteConfig, addTemplate, hasNuxtModule, addPlugin, addComponent, addServerHandler, addPrerenderRoutes, defineNuxtModule, createResolver, addImports, addServerImports, addTypeTemplate } from '@nuxt/kit';
import { hash } from 'ohash';
import { join, isAbsolute, dirname, extname, relative, resolve, normalize } from 'pathe';
import htmlTags from '@nuxtjs/mdc/runtime/parser/utils/html-tags-list';
import { pascalCase, camelCase, kebabCase } from 'scule';
import defu$1, { defu as defu$2, createDefu } from 'defu';
import { getOrderedSchemaKeys, getCollectionFieldsTypes, describeProperty } from '../dist/runtime/internal/schema.js';
import { withLeadingSlash, withoutTrailingSlash, isRelative, withTrailingSlash } from 'ufo';
import { glob } from 'tinyglobby';
import { createWriteStream } from 'node:fs';
import { pipeline } from 'node:stream';
import { promisify } from 'node:util';
import { extract } from 'tar';
import { readGitConfig } from 'pkg-types';
import gitUrlParse from 'git-url-parse';
import crypto from 'node:crypto';
import chokidar from 'chokidar';
import micromatch from 'micromatch';
import { WebSocketServer } from 'ws';
import { listen } from 'listhen';
import cloudflareD1Connector from 'db0/connectors/cloudflare-d1';
import { isWebContainer } from '@webcontainer/env';
import { addDependency } from 'nypm';
import { createContext } from 'unctx';
import { dirname as dirname$1 } from 'node:path';
import { parseMarkdown, createShikiHighlighter, rehypeHighlight } from '@nuxtjs/mdc/runtime';
import { createJiti } from 'jiti';
import { createOnigurumaEngine } from 'shiki/engine/oniguruma';
import { visit } from 'unist-util-visit';
import { unified } from 'unified';
import { toString } from 'mdast-util-to-string';
import { postprocess, preprocess } from 'micromark';
import { stringifyPosition } from 'unist-util-stringify-position';
import { markdownLineEnding, markdownSpace } from 'micromark-util-character';
import { push, splice } from 'micromark-util-chunked';
import { resolveAll } from 'micromark-util-resolve-all';
import { normalizeUri } from 'micromark-util-sanitize-uri';
import { fromHast } from 'minimark/hast';
import slugify from 'slugify';
import { parseFrontMatter } from 'remark-mdc';
import { destr } from 'destr';
import { gzip } from 'node:zlib';
import { genDynamicImport } from 'knitwork';
import { compile } from 'json-schema-to-typescript';
import { getComponentMeta } from 'nuxt-component-meta/parser';
import { propsToJsonSchema } from 'nuxt-component-meta/utils';
import { createDefineConfig, watchConfig, loadConfig } from 'c12';
import { zodToJsonSchema, ignoreOverride } from 'zod-to-json-schema';
import { z as z$1 } from 'zod';

const version = "3.7.1";

async function downloadRepository(url, cwd, { headers } = {}) {
  const tarFile = join(cwd, ".content.clone.tar.gz");
  const cacheFile = join(cwd, ".content.cache.json");
  const cache = await readFile(cacheFile, "utf8").then((d) => JSON.parse(d)).catch(() => null);
  if (cache) {
    const response = await fetch(url, { method: "HEAD", headers });
    const etag = response.headers.get("etag");
    if (etag === cache.etag) {
      await writeFile(cacheFile, JSON.stringify({
        ...cache,
        updatedAt: (/* @__PURE__ */ new Date()).toISOString()
      }, null, 2));
      return;
    }
  }
  await mkdir(cwd, { recursive: true });
  try {
    const response = await fetch(url, { headers });
    const stream = createWriteStream(tarFile);
    await promisify(pipeline)(response.body, stream);
    await extract({
      file: tarFile,
      cwd,
      onentry(entry) {
        entry.path = entry.path.split("/").splice(1).join("/");
      }
    });
    await writeFile(cacheFile, JSON.stringify({
      url,
      etag: response.headers.get("etag"),
      createdAt: (/* @__PURE__ */ new Date()).toISOString(),
      updatedAt: (/* @__PURE__ */ new Date()).toISOString()
    }, null, 2));
  } finally {
    await rm(tarFile, { force: true });
  }
}
function parseGitHubUrl(url) {
  const regex = /https:\/\/github\.com\/([^/]+)\/([^/]+)(?:\/tree\/([^/]+))?(?:\/(.+))?/;
  const match = url.match(regex);
  if (match) {
    const org = match[1];
    const repo = match[2];
    let branch = match[3] || "main";
    let path = match[4] || "";
    if (["fix", "feat", "chore", "test", "docs"].includes(branch)) {
      const pathParts = path.split("/");
      branch = join(branch, pathParts[0] || "");
      path = pathParts.slice(1).join("/");
    }
    return {
      org,
      repo,
      branch,
      path
    };
  }
  return null;
}
function parseBitBucketUrl(url) {
  const bitbucketRegex = /https:\/\/bitbucket\.org\/([^/]+)\/([^/]+)(?:\/src\/([^/]+))?(?:\/(.+))?/;
  const bitbucketMatch = url.match(bitbucketRegex);
  if (bitbucketMatch) {
    const org = bitbucketMatch[1];
    const repo = bitbucketMatch[2];
    let branch = bitbucketMatch[3] || "main";
    let path = bitbucketMatch[4] || "";
    if (["fix", "feat", "chore", "test", "docs"].includes(branch)) {
      const pathParts = path.split("/");
      branch = join(branch, pathParts[0] || "");
      path = pathParts.slice(1).join("/");
    }
    return {
      org,
      repo,
      branch,
      path
    };
  }
  return null;
}
async function getLocalGitInfo(rootDir) {
  const remote = await getLocalGitRemote(rootDir);
  if (!remote) {
    return;
  }
  const { name, owner, source } = gitUrlParse(remote);
  const url = `https://${source}/${owner}/${name}`;
  return {
    name,
    owner,
    url
  };
}
function getGitEnv() {
  const envInfo = {
    // Provider
    provider: process.env.VERCEL_GIT_PROVIDER || (process.env.GITHUB_SERVER_URL ? "github" : void 0) || "",
    // Owner
    owner: process.env.VERCEL_GIT_REPO_OWNER || process.env.GITHUB_REPOSITORY_OWNER || process.env.CI_PROJECT_PATH?.split("/").shift() || "",
    // Name
    name: process.env.VERCEL_GIT_REPO_SLUG || process.env.GITHUB_REPOSITORY?.split("/").pop() || process.env.CI_PROJECT_PATH?.split("/").splice(1).join("/") || "",
    // Url
    url: process.env.REPOSITORY_URL || ""
    // netlify
  };
  if (!envInfo.url && envInfo.provider && envInfo.owner && envInfo.name) {
    envInfo.url = `https://${envInfo.provider}.com/${envInfo.owner}/${envInfo.name}`;
  }
  if (!envInfo.name && !envInfo.owner && envInfo.url) {
    try {
      const { name, owner } = gitUrlParse(envInfo.url);
      envInfo.name = name;
      envInfo.owner = owner;
    } catch {
    }
  }
  return {
    name: envInfo.name,
    owner: envInfo.owner,
    url: envInfo.url
  };
}
async function getLocalGitRemote(dir) {
  try {
    const parsed = await readGitConfig(dir);
    if (!parsed) {
      return;
    }
    return parsed.remote?.["origin"]?.url;
  } catch {
  }
}

const nuxtContentContext = {
  zod3: {
    toJSONSchema: (_schema, _name) => {
      throw new Error(
        "It seems you are using Zod version 3 for collection schema, but Zod is not installed, Nuxt Content does not ship with zod, install `zod` and `zod-to-json-schema` and it will work."
      );
    }
  },
  zod4: {
    toJSONSchema: (_schema, _name) => {
      throw new Error(
        "It seems you are using Zod version 4 for collection schema, but Zod is not installed, Nuxt Content does not ship with zod, install `zod` and it will work."
      );
    }
  },
  valibot: {
    toJSONSchema: (_schema, _name) => {
      throw new Error(
        "It seems you are using Valibot for collection schema, but Valibot is not installed, Nuxt Content does not ship with valibot, install `valibot` and `@valibot/to-json-schema` and it will work."
      );
    }
  },
  unknown: {
    toJSONSchema: (_schema, _name) => {
      throw new Error("Unknown schema vendor");
    }
  },
  set: (key, value) => {
    nuxtContentContext[key] = value;
  },
  get: (key) => {
    return nuxtContentContext[key];
  }
};
const ctx = createContext();
ctx.set(nuxtContentContext);
const nuxtContentContext$1 = ctx.use;

async function isPackageInstalled(packageName) {
  try {
    await import(packageName);
    return true;
  } catch {
    return false;
  }
}
async function ensurePackageInstalled(pkg) {
  if (!await isPackageInstalled(pkg)) {
    logger.error(`Nuxt Content requires \`${pkg}\` module to operate.`);
    const confirm = await logger.prompt(`Do you want to install \`${pkg}\` package?`, {
      type: "confirm",
      name: "confirm",
      initial: true
    });
    if (!confirm) {
      logger.error(`Nuxt Content requires \`${pkg}\` module to operate. Please install \`${pkg}\` package manually and try again. \`npm install ${pkg}\``);
      process.exit(1);
    }
    await addDependency(pkg, {
      cwd: tryUseNuxt()?.options.rootDir
    });
  }
}
function isNodeSqliteAvailable() {
  try {
    const module = globalThis.process?.getBuiltinModule?.("node:sqlite");
    if (module) {
      const originalEmit = process.emit;
      process.emit = function(...args) {
        const name = args[0];
        const data = args[1];
        if (name === `warning` && typeof data === `object` && data.name === `ExperimentalWarning` && data.message.includes(`SQLite is an experimental feature`)) {
          return false;
        }
        return originalEmit.apply(process, args);
      };
      return true;
    }
    return false;
  } catch {
    return false;
  }
}
async function initiateValidatorsContext() {
  if (await isPackageInstalled("valibot") && await isPackageInstalled("@valibot/to-json-schema")) {
    nuxtContentContext$1().set("valibot", await import('./chunks/valibot.mjs'));
  }
  if (await isPackageInstalled("zod")) {
    nuxtContentContext$1().set("zod3", await Promise.resolve().then(function () { return zod3; }));
    nuxtContentContext$1().set("zod4", await import('./chunks/zod4.mjs'));
  }
}

const databaseVersion = "v3.5.0";
async function refineDatabaseConfig(database, opts) {
  if (database.type === "d1") {
    if (!("bindingName" in database)) {
      database.bindingName = database.binding;
    }
  }
  if (database.type === "sqlite") {
    const path = isAbsolute(database.filename) ? database.filename : join(opts.rootDir, database.filename);
    await mkdir(dirname(path), { recursive: true }).catch(() => {
    });
    if (opts.updateSqliteFileName) {
      database.filename = path;
    }
  }
}
async function resolveDatabaseAdapter(adapter, opts) {
  const databaseConnectors = {
    nodesqlite: "db0/connectors/node-sqlite",
    bunsqlite: opts.resolver.resolve("./runtime/internal/connectors/bun-sqlite"),
    postgres: "db0/connectors/postgresql",
    libsql: "db0/connectors/libsql/web",
    d1: "db0/connectors/cloudflare-d1"
  };
  adapter = adapter || "sqlite";
  if (adapter === "sqlite" && process.versions.bun) {
    return databaseConnectors.bunsqlite;
  }
  if (adapter === "sqlite") {
    return await findBestSqliteAdapter({ sqliteConnector: opts.sqliteConnector, resolver: opts.resolver });
  }
  return databaseConnectors[adapter];
}
async function getDatabase(database, opts) {
  if (database.type === "d1") {
    return cloudflareD1Connector({ bindingName: database.bindingName });
  }
  return import(await findBestSqliteAdapter(opts)).then((m) => {
    const connector = m.default || m;
    return connector({ path: database.filename });
  });
}
const _localDatabase = {};
async function getLocalDatabase(database, { connector, sqliteConnector } = {}) {
  const databaseLocation = database.type === "sqlite" ? database.filename : database.bindingName;
  const db = _localDatabase[databaseLocation] || connector || await getDatabase(database, { sqliteConnector });
  const cacheCollection = {
    tableName: "_development_cache",
    extendedSchema: {
      $schema: "http://json-schema.org/draft-07/schema#",
      $ref: "#/definitions/cache",
      definitions: {
        cache: {
          type: "object",
          properties: {
            id: { type: "string" },
            value: { type: "string" },
            checksum: { type: "string" }
          },
          required: ["id", "value", "checksum"]
        }
      }
    }};
  if (!_localDatabase[databaseLocation]) {
    _localDatabase[databaseLocation] = db;
    let dropCacheTable = false;
    try {
      dropCacheTable = await db.prepare("SELECT * FROM _development_cache WHERE id = ?").get("__DATABASE_VERSION__").then((row) => row?.value !== databaseVersion);
    } catch {
      dropCacheTable = true;
    }
    const initQueries = generateCollectionTableDefinition(cacheCollection, { drop: Boolean(dropCacheTable) });
    for (const query of initQueries.split("\n")) {
      await db.exec(query);
    }
    if (dropCacheTable) {
      await db.exec(generateCollectionInsert(cacheCollection, { id: "__DATABASE_VERSION__", value: databaseVersion, checksum: databaseVersion }).queries[0]);
    }
  }
  const fetchDevelopmentCache = async () => {
    const result = await db.prepare("SELECT * FROM _development_cache").all();
    return result.reduce((acc, cur) => ({ ...acc, [cur.id]: cur }), {});
  };
  const fetchDevelopmentCacheForKey = async (id) => {
    return await db.prepare("SELECT * FROM _development_cache WHERE id = ?").get(id);
  };
  const insertDevelopmentCache = async (id, value, checksum) => {
    deleteDevelopmentCache(id);
    const insert = generateCollectionInsert(cacheCollection, { id, value, checksum });
    for (const query of insert.queries) {
      await db.exec(query);
    }
  };
  const deleteDevelopmentCache = async (id) => {
    db.prepare(`DELETE FROM _development_cache WHERE id = ?`).run(id);
  };
  const dropContentTables = async () => {
    const tables = await db.prepare("SELECT name FROM sqlite_master WHERE type = ? AND name LIKE ?").all("table", "_content_%");
    for (const { name } of tables) {
      db.exec(`DROP TABLE ${name}`);
    }
  };
  return {
    database: db,
    async exec(sql) {
      db.exec(sql);
    },
    close() {
      Reflect.deleteProperty(_localDatabase, databaseLocation);
    },
    fetchDevelopmentCache,
    fetchDevelopmentCacheForKey,
    insertDevelopmentCache,
    deleteDevelopmentCache,
    dropContentTables
  };
}
async function findBestSqliteAdapter(opts) {
  if (process.versions.bun) {
    return opts.resolver ? opts.resolver.resolve("./runtime/internal/connectors/bun-sqlite") : "db0/connectors/bun-sqlite";
  }
  if (opts.sqliteConnector === "native" && isNodeSqliteAvailable()) {
    return opts.resolver ? opts.resolver.resolve("./runtime/internal/connectors/node-sqlite") : "db0/connectors/node-sqlite";
  }
  if (opts.sqliteConnector === "sqlite3") {
    return "db0/connectors/sqlite3";
  }
  if (opts.sqliteConnector === "better-sqlite3") {
    await ensurePackageInstalled("better-sqlite3");
    return "db0/connectors/better-sqlite3";
  }
  if (isWebContainer()) {
    await ensurePackageInstalled("sqlite3");
    return "db0/connectors/sqlite3";
  }
  await ensurePackageInstalled("better-sqlite3");
  return "db0/connectors/better-sqlite3";
}

const defineTransformer = (transformer) => {
  return transformer;
};

function createTokenizer(parser, initialize, from) {
  let point = Object.assign(
    {
      line: 1,
      column: 1,
      offset: 0
    },
    {
      _index: 0,
      _bufferIndex: -1
    }
  );
  const columnStart = {};
  const resolveAllConstructs = [];
  let chunks = [];
  let stack = [];
  const effects = {
    consume,
    enter,
    exit,
    attempt: constructFactory(onsuccessfulconstruct),
    check: constructFactory(onsuccessfulcheck),
    interrupt: constructFactory(onsuccessfulcheck, {
      interrupt: true
    })
  };
  const context = {
    previous: null,
    code: null,
    containerState: {},
    events: [],
    parser,
    sliceStream,
    sliceSerialize,
    now,
    defineSkip,
    write
  };
  let state = initialize.tokenize.call(context, effects);
  if (initialize.resolveAll) {
    resolveAllConstructs.push(initialize);
  }
  return context;
  function write(slice) {
    chunks = push(chunks, slice);
    main();
    if (chunks[chunks.length - 1] !== null) {
      return [];
    }
    addResult(initialize, 0);
    context.events = resolveAll(resolveAllConstructs, context.events, context);
    return context.events;
  }
  function sliceSerialize(token, expandTabs) {
    return serializeChunks(sliceStream(token), expandTabs);
  }
  function sliceStream(token) {
    return sliceChunks(chunks, token);
  }
  function now() {
    return Object.assign({}, point);
  }
  function defineSkip(value) {
    columnStart[value.line] = value.column;
    accountForPotentialSkip();
  }
  function main() {
    let chunkIndex;
    while (point._index < chunks.length) {
      const chunk = chunks[point._index];
      if (typeof chunk === "string") {
        chunkIndex = point._index;
        if (point._bufferIndex < 0) {
          point._bufferIndex = 0;
        }
        while (point._index === chunkIndex && point._bufferIndex < chunk.length) {
          go(chunk.charCodeAt(point._bufferIndex));
        }
      } else {
        go(chunk);
      }
    }
  }
  function go(code) {
    state = state(code);
  }
  function consume(code) {
    if (markdownLineEnding(code)) {
      point.line++;
      point.column = 1;
      point.offset += code === -3 ? 2 : 1;
      accountForPotentialSkip();
    } else if (code !== -1) {
      point.column++;
      point.offset++;
    }
    if (point._bufferIndex < 0) {
      point._index++;
    } else {
      point._bufferIndex++;
      if (point._bufferIndex === chunks[point._index].length) {
        point._bufferIndex = -1;
        point._index++;
      }
    }
    context.previous = code;
  }
  function enter(type, fields) {
    const token = fields || {};
    token.type = type;
    token.start = now();
    context.events.push(["enter", token, context]);
    stack.push(token);
    return token;
  }
  function exit(type) {
    const token = stack.pop();
    token.end = now();
    context.events.push(["exit", token, context]);
    return token;
  }
  function onsuccessfulconstruct(construct, info) {
    addResult(construct, info.from);
  }
  function onsuccessfulcheck(_, info) {
    info.restore();
  }
  function constructFactory(onreturn, fields) {
    return hook;
    function hook(constructs, returnState, bogusState) {
      let listOfConstructs;
      let constructIndex;
      let currentConstruct;
      let info;
      return Array.isArray(constructs) ? (
        /* c8 ignore next 1 */
        handleListOfConstructs(constructs)
      ) : "tokenize" in constructs ? handleListOfConstructs([constructs]) : handleMapOfConstructs(constructs);
      function handleMapOfConstructs(map) {
        return start;
        function start(code) {
          const def = code !== null && map[code];
          const all = code !== null && map.null;
          const list = [
            // To do: add more extension tests.
            /* c8 ignore next 2 */
            ...Array.isArray(def) ? def : def ? [def] : [],
            ...Array.isArray(all) ? all : all ? [all] : []
          ];
          return handleListOfConstructs(list)(code);
        }
      }
      function handleListOfConstructs(list) {
        listOfConstructs = list;
        constructIndex = 0;
        if (list.length === 0) {
          return bogusState;
        }
        return handleConstruct(list[constructIndex]);
      }
      function handleConstruct(construct) {
        return start;
        function start(code) {
          info = store();
          currentConstruct = construct;
          if (!construct.partial) {
            context.currentConstruct = construct;
          }
          if (construct.name && context.parser.constructs.disable.null.includes(construct.name)) {
            return nok();
          }
          return construct.tokenize.call(
            // If we do have fields, create an object w/ `context` as its
            // prototype.
            // This allows a “live binding”, which is needed for `interrupt`.
            fields ? Object.assign(Object.create(context), fields) : context,
            effects,
            ok,
            nok
          )(code);
        }
      }
      function ok(code) {
        onreturn(currentConstruct, info);
        return returnState;
      }
      function nok(code) {
        info.restore();
        if (++constructIndex < listOfConstructs.length) {
          return handleConstruct(listOfConstructs[constructIndex]);
        }
        return bogusState;
      }
    }
  }
  function addResult(construct, from2) {
    if (construct.resolveAll && !resolveAllConstructs.includes(construct)) {
      resolveAllConstructs.push(construct);
    }
    if (construct.resolve) {
      splice(
        context.events,
        from2,
        context.events.length - from2,
        construct.resolve(context.events.slice(from2), context)
      );
    }
    if (construct.resolveTo) {
      context.events = construct.resolveTo(context.events, context);
    }
  }
  function store() {
    const startPoint = now();
    const startPrevious = context.previous;
    const startCurrentConstruct = context.currentConstruct;
    const startEventsIndex = context.events.length;
    const startStack = Array.from(stack);
    return {
      restore,
      from: startEventsIndex
    };
    function restore() {
      point = startPoint;
      context.previous = startPrevious;
      context.currentConstruct = startCurrentConstruct;
      context.events.length = startEventsIndex;
      stack = startStack;
      accountForPotentialSkip();
    }
  }
  function accountForPotentialSkip() {
    if (point.line in columnStart && point.column < 2) {
      point.column = columnStart[point.line];
      point.offset += columnStart[point.line] - 1;
    }
  }
}
function sliceChunks(chunks, token) {
  const startIndex = token.start._index;
  const startBufferIndex = token.start._bufferIndex;
  const endIndex = token.end._index;
  const endBufferIndex = token.end._bufferIndex;
  let view;
  if (startIndex === endIndex) {
    view = [chunks[startIndex].slice(startBufferIndex, endBufferIndex)];
  } else {
    view = chunks.slice(startIndex, endIndex);
    if (startBufferIndex > -1) {
      view[0] = view[0].slice(startBufferIndex);
    }
    if (endBufferIndex > 0) {
      view.push(chunks[endIndex].slice(0, endBufferIndex));
    }
  }
  return view;
}
function serializeChunks(chunks, expandTabs) {
  let index = -1;
  const result = [];
  let atTab;
  while (++index < chunks.length) {
    const chunk = chunks[index];
    let value;
    if (typeof chunk === "string") {
      value = chunk;
    } else
      switch (chunk) {
        case -5: {
          value = "\r";
          break;
        }
        case -4: {
          value = "\n";
          break;
        }
        case -3: {
          value = "\r\n";
          break;
        }
        case -2: {
          value = expandTabs ? " " : "	";
          break;
        }
        case -1: {
          if (!expandTabs && atTab) continue;
          value = " ";
          break;
        }
        default: {
          value = String.fromCharCode(chunk);
        }
      }
    atTab = chunk === -2;
    result.push(value);
  }
  return result.join("");
}

function initializeDocument(effects) {
  const self = this;
  const delimiter = (this.parser.delimiter || ",").charCodeAt(0);
  return enterRow;
  function enterRow(code) {
    return effects.attempt(
      { tokenize: attemptLastLine },
      (code2) => {
        effects.consume(code2);
        return enterRow;
      },
      (code2) => {
        effects.enter("row");
        return enterColumn(code2);
      }
    )(code);
  }
  function enterColumn(code) {
    effects.enter("column");
    return content(code);
  }
  function content(code) {
    if (code === null) {
      effects.exit("column");
      effects.exit("row");
      effects.consume(code);
      return content;
    }
    if (code === 34) {
      return quotedData(code);
    }
    if (code === delimiter) {
      if (self.previous === delimiter || markdownLineEnding(self.previous) || self.previous === null) {
        effects.enter("data");
        effects.exit("data");
      }
      effects.exit("column");
      effects.enter("columnSeparator");
      effects.consume(code);
      effects.exit("columnSeparator");
      effects.enter("column");
      return content;
    }
    if (markdownLineEnding(code)) {
      effects.exit("column");
      effects.enter("newline");
      effects.consume(code);
      effects.exit("newline");
      effects.exit("row");
      return enterRow;
    }
    return data(code);
  }
  function data(code) {
    effects.enter("data");
    return dataChunk(code);
  }
  function dataChunk(code) {
    if (code === null || markdownLineEnding(code) || code === delimiter) {
      effects.exit("data");
      return content(code);
    }
    if (code === 92) {
      return escapeCharacter(code);
    }
    effects.consume(code);
    return dataChunk;
  }
  function escapeCharacter(code) {
    effects.consume(code);
    return function(code2) {
      effects.consume(code2);
      return content;
    };
  }
  function quotedData(code) {
    effects.enter("quotedData");
    effects.enter("quotedDataChunk");
    effects.consume(code);
    return quotedDataChunk;
  }
  function quotedDataChunk(code) {
    if (code === 92) {
      return escapeCharacter(code);
    }
    if (code === 34) {
      return effects.attempt(
        { tokenize: attemptDoubleQuote },
        (code2) => {
          effects.exit("quotedDataChunk");
          effects.enter("quotedDataChunk");
          return quotedDataChunk(code2);
        },
        (code2) => {
          effects.consume(code2);
          effects.exit("quotedDataChunk");
          effects.exit("quotedData");
          return content;
        }
      )(code);
    }
    effects.consume(code);
    return quotedDataChunk;
  }
}
function attemptDoubleQuote(effects, ok, nok) {
  return startSequence;
  function startSequence(code) {
    if (code !== 34) {
      return nok(code);
    }
    effects.enter("quoteFence");
    effects.consume(code);
    return sequence;
  }
  function sequence(code) {
    if (code !== 34) {
      return nok(code);
    }
    effects.consume(code);
    effects.exit("quoteFence");
    return (code2) => ok(code2);
  }
}
function attemptLastLine(effects, ok, nok) {
  return enterLine;
  function enterLine(code) {
    if (!markdownSpace(code) && code !== null) {
      return nok(code);
    }
    effects.enter("emptyLine");
    return continueLine(code);
  }
  function continueLine(code) {
    if (markdownSpace(code)) {
      effects.consume(code);
      return continueLine;
    }
    if (code === null) {
      effects.exit("emptyLine");
      return ok(code);
    }
    return nok(code);
  }
}
const parse = (options) => {
  return createTokenizer(
    { ...options },
    { tokenize: initializeDocument });
};

const own = {}.hasOwnProperty;
const initialPoint = {
  line: 1,
  column: 1,
  offset: 0
};
const fromCSV = function(value, encoding, options) {
  if (typeof encoding !== "string") {
    options = encoding;
    encoding = void 0;
  }
  return compiler()(
    postprocess(
      parse(options).write(preprocess()(value, encoding, true))
    )
  );
};
function compiler() {
  const config = {
    enter: {
      column: opener(openColumn),
      row: opener(openRow),
      data: onenterdata,
      quotedData: onenterdata
    },
    exit: {
      row: closer(),
      column: closer(),
      data: onexitdata,
      quotedData: onexitQuotedData
    }
  };
  return compile;
  function compile(events) {
    const tree = {
      type: "root",
      children: []
    };
    const stack = [tree];
    const tokenStack = [];
    const context = {
      stack,
      tokenStack,
      config,
      enter,
      exit,
      resume
    };
    let index = -1;
    while (++index < events.length) {
      const handler = config[events[index][0]];
      if (own.call(handler, events[index][1].type)) {
        handler[events[index][1].type].call(
          Object.assign(
            {
              sliceSerialize: events[index][2].sliceSerialize
            },
            context
          ),
          events[index][1]
        );
      }
    }
    if (tokenStack.length > 0) {
      const tail = tokenStack[tokenStack.length - 1];
      const handler = tail[1] || defaultOnError;
      handler.call(context, void 0, tail[0]);
    }
    tree.position = {
      start: point(
        events.length > 0 ? events[0][1].start : initialPoint
      ),
      end: point(
        events.length > 0 ? events[events.length - 2][1].end : initialPoint
      )
    };
    return tree;
  }
  function point(d) {
    return {
      line: d.line,
      column: d.column,
      offset: d.offset
    };
  }
  function opener(create, and) {
    return open;
    function open(token) {
      enter.call(this, create(token), token);
    }
  }
  function enter(node, token, errorHandler) {
    const parent = this.stack[this.stack.length - 1];
    parent.children.push(node);
    this.stack.push(node);
    this.tokenStack.push([token, errorHandler]);
    node.position = {
      start: point(token.start)
    };
    return node;
  }
  function closer(and) {
    return close;
    function close(token) {
      exit.call(this, token);
    }
  }
  function exit(token, onExitError) {
    const node = this.stack.pop();
    const open = this.tokenStack.pop();
    if (!open) {
      throw new Error(
        "Cannot close `" + token.type + "` (" + stringifyPosition({
          start: token.start,
          end: token.end
        }) + "): it\u2019s not open"
      );
    } else if (open[0].type !== token.type) {
      if (onExitError) {
        onExitError.call(this, token, open[0]);
      } else {
        const handler = open[1] || defaultOnError;
        handler.call(this, token, open[0]);
      }
    }
    node.position.end = point(token.end);
    return node;
  }
  function resume() {
    return toString(this.stack.pop());
  }
  function onenterdata(token) {
    const parent = this.stack[this.stack.length - 1];
    let tail = parent.children[parent.children.length - 1];
    if (!tail || tail.type !== "text") {
      tail = text();
      tail.position = {
        start: point(token.start)
      };
      parent.children.push(tail);
    }
    this.stack.push(tail);
  }
  function onexitdata(token) {
    const tail = this.stack.pop();
    tail.value += this.sliceSerialize(token).trim().replace(/""/g, '"');
    tail.position.end = point(token.end);
  }
  function onexitQuotedData(token) {
    const tail = this.stack.pop();
    const value = this.sliceSerialize(token);
    tail.value += this.sliceSerialize(token).trim().substring(1, value.length - 1).replace(/""/g, '"');
    tail.position.end = point(token.end);
  }
  function text() {
    return {
      type: "text",
      value: ""
    };
  }
  function openColumn() {
    return {
      type: "column",
      children: []
    };
  }
  function openRow() {
    return {
      type: "row",
      children: []
    };
  }
}
function defaultOnError(left, right) {
  if (left) {
    throw new Error([
      "Cannot close `",
      left.type,
      "` (",
      stringifyPosition({
        start: left.start,
        end: left.end
      }),
      "): a different token (`",
      right.type,
      "`, ",
      stringifyPosition({
        start: right.start,
        end: right.end
      }),
      ") is open"
    ].join(""));
  } else {
    throw new Error(
      "Cannot close document, a token (`" + right.type + "`, " + stringifyPosition({
        start: right.start,
        end: right.end
      }) + ") is still open"
    );
  }
}

function csvParse(options) {
  const parser = (doc) => {
    return fromCSV(doc, options);
  };
  Object.assign(this, { Parser: parser });
  const toJsonObject = (tree) => {
    const [header, ...rows] = tree.children;
    const columns = header.children.map((col) => col.children[0].value);
    const data = rows.map((row) => {
      return row.children.reduce((acc, col, i) => {
        acc[String(columns[i])] = col.children[0]?.value;
        return acc;
      }, {});
    });
    return data;
  };
  const toJsonArray = (tree) => {
    const data = tree.children.map((row) => {
      return row.children.map((col) => col.children[0]?.value);
    });
    return data;
  };
  const compiler = (doc) => {
    if (options.json) {
      return toJsonObject(doc);
    }
    return toJsonArray(doc);
  };
  Object.assign(this, { Compiler: compiler });
}
const csv = defineTransformer({
  name: "csv",
  extensions: [".csv"],
  parse: async (file, options = {}) => {
    const stream = unified().use(csvParse, {
      delimiter: ",",
      json: true,
      ...options
    });
    const { result } = await stream.process(file.body);
    return {
      id: file.id,
      body: result
    };
  }
});

const SEMVER_REGEX = /^\d+(?:\.\d+)*(?:\.x)?$/;
const defaultOptions = {
  slugifyOptions: {
    lower: true
  }
};
const pathMeta = defineTransformer({
  name: "path-meta",
  extensions: [".*"],
  transform(content, options = {}) {
    const opts = defu$1(options, defaultOptions);
    const { basename, extension, stem } = describeId(content.id);
    const filePath = generatePath(stem, opts);
    return {
      path: filePath,
      ...content,
      title: content.title || generateTitle(refineUrlPart(basename)),
      stem,
      extension
    };
  }
});
const generatePath = (path, { forceLeadingSlash = true, slugifyOptions = {} } = {}) => {
  path = path.split("/").map((part) => slugify(refineUrlPart(part), slugifyOptions)).join("/");
  return forceLeadingSlash ? withLeadingSlash(withoutTrailingSlash(path)) : path;
};
const generateTitle = (path) => path.split(/[\s-]/g).map(pascalCase).join(" ");
function refineUrlPart(name) {
  name = name.split(/[/:]/).pop();
  if (SEMVER_REGEX.test(name)) {
    return name;
  }
  return name.replace(/(\d+\.)?(.*)/, "$2").replace(/^index(\.draft)?$/, "").replace(/\.draft$/, "");
}
const describeId = (id) => {
  const [source, ...parts] = id.split(/[:/]/);
  const [, basename, extension] = parts[parts.length - 1]?.match(/(.*)\.([^.]+)$/) || [];
  if (basename) {
    parts[parts.length - 1] = basename;
  }
  const stem = (parts || []).join("/");
  return {
    source,
    stem,
    extension,
    basename: basename || ""
  };
};

const markdown = defineTransformer({
  name: "markdown",
  extensions: [".md"],
  parse: async (file, options = {}) => {
    const config = { ...options };
    config.rehypePlugins = await importPlugins(config.rehypePlugins);
    config.remarkPlugins = await importPlugins(config.remarkPlugins);
    const highlight = options.highlight ? {
      ...options.highlight,
      // Pass only when it's an function. String values are handled by `@nuxtjs/mdc`
      highlighter: typeof options.highlight?.highlighter === "function" ? options.highlight.highlighter : void 0
    } : void 0;
    const parsed = await parseMarkdown(file.body, {
      ...config,
      highlight,
      toc: config.toc,
      remark: { plugins: config.remarkPlugins },
      rehype: {
        plugins: config.rehypePlugins,
        options: { handlers: { link } }
      }
    }, {
      fileOptions: file
    });
    if (options.compress) {
      return {
        ...parsed.data,
        excerpt: parsed.excerpt ? fromHast(parsed.excerpt) : void 0,
        body: {
          ...fromHast(parsed.body),
          toc: parsed.toc
        },
        id: file.id
      };
    }
    return {
      ...parsed.data,
      excerpt: parsed.excerpt,
      body: {
        ...parsed.body,
        toc: parsed.toc
      },
      id: file.id
    };
  }
});
async function importPlugins(plugins = {}) {
  const resolvedPlugins = {};
  for (const [name, plugin] of Object.entries(plugins)) {
    if (plugin) {
      resolvedPlugins[name] = {
        instance: plugin.instance || await import(
          /* @vite-ignore */
          name
        ).then((m) => m.default || m),
        options: plugin.options || {}
      };
    } else {
      resolvedPlugins[name] = false;
    }
  }
  return resolvedPlugins;
}
function link(state, node) {
  const properties = {
    ...node.attributes || {},
    href: normalizeUri(normaliseLink(node.url))
  };
  if (node.title !== null && node.title !== void 0) {
    properties.title = node.title;
  }
  const result = {
    type: "element",
    tagName: "a",
    properties,
    children: state.all(node)
  };
  state.patch(node, result);
  return state.applyData(node, result);
}
function normaliseLink(link2) {
  const match = link2.match(/#.+$/);
  const hash = match ? match[0] : "";
  if (link2.replace(/#.+$/, "").endsWith(".md") && (isRelative(link2) || !/^https?/.test(link2) && !link2.startsWith("/"))) {
    return generatePath(link2.replace(".md" + hash, ""), { forceLeadingSlash: false }) + hash;
  } else {
    return link2;
  }
}

const yaml = defineTransformer({
  name: "Yaml",
  extensions: [".yml", ".yaml"],
  parse: (file) => {
    const { id, body } = file;
    const { data } = parseFrontMatter(`---
${body}
---`);
    let parsed = data;
    if (Array.isArray(data)) {
      console.warn(`YAML array is not supported in ${id}, moving the array into the \`body\` key`);
      parsed = { body: data };
    }
    return {
      ...parsed,
      body: parsed.body || parsed,
      id
    };
  }
});

const json = defineTransformer({
  name: "Json",
  extensions: [".json"],
  parse: async (file) => {
    const { id, body } = file;
    let parsed;
    if (typeof body === "string") {
      parsed = destr(body);
    } else {
      parsed = body;
    }
    if (Array.isArray(parsed)) {
      console.warn(`JSON array is not supported in ${id}, moving the array into the \`body\` key`);
      parsed = {
        body: parsed
      };
    }
    return {
      ...parsed,
      body: parsed.body || parsed,
      id
    };
  }
});

const TRANSFORMERS = [
  csv,
  markdown,
  json,
  yaml,
  pathMeta
];
function getParser(ext, additionalTransformers = []) {
  let parser = additionalTransformers.find((p) => ext.match(new RegExp(p.extensions.join("|"), "i")) && p.parse);
  if (!parser) {
    parser = TRANSFORMERS.find((p) => ext.match(new RegExp(p.extensions.join("|"), "i")) && p.parse);
  }
  return parser;
}
function getTransformers(ext, additionalTransformers = []) {
  return [
    ...additionalTransformers.filter((p) => ext.match(new RegExp(p.extensions.join("|"), "i")) && p.transform),
    ...TRANSFORMERS.filter((p) => ext.match(new RegExp(p.extensions.join("|"), "i")) && p.transform)
  ];
}
async function transformContent(file, options = {}) {
  const { transformers = [] } = options;
  const ext = file.extension || extname(file.id);
  const parser = getParser(ext, transformers);
  if (!parser) {
    throw new Error(`\`${ext}\` files are not supported.`);
  }
  const parserOptions = options[camelCase(parser.name)] || {};
  const parsed = await parser.parse(file, parserOptions);
  const matchedTransformers = getTransformers(ext, transformers);
  const result = await matchedTransformers.reduce(async (prev, cur) => {
    const next = await prev || parsed;
    const transformOptions = options[camelCase(cur.name)];
    if (transformOptions === false) {
      return next;
    }
    return cur.transform(next, transformOptions || {});
  }, Promise.resolve(parsed));
  return result;
}

let parserOptions = {
  mdcConfigs: []
};
function setParserOptions(opts) {
  parserOptions = defu$2(opts, parserOptions);
}
let highlightPlugin;
let highlightPluginPromise;
async function getHighlightPluginInstance(options) {
  const key = hash(JSON.stringify(options || {}));
  if (highlightPlugin && highlightPlugin.key !== key) {
    highlightPlugin = void 0;
    highlightPluginPromise = void 0;
  }
  if (!highlightPlugin) {
    highlightPluginPromise = highlightPluginPromise || _getHighlightPlugin(key, options);
    await highlightPluginPromise;
  }
  return highlightPlugin;
}
async function _getHighlightPlugin(key, options) {
  const langs = Array.from(/* @__PURE__ */ new Set(["bash", "html", "mdc", "vue", "yml", "scss", "ts", "typescript", ...options.langs || []]));
  const themesObject = typeof options.theme === "string" ? { default: options.theme } : options.theme || { default: "material-theme-palenight" };
  const bundledThemes = await Promise.all(Object.entries(themesObject).map(async ([name, theme]) => [
    name,
    typeof theme === "string" ? await import(`shiki/themes/${theme}.mjs`).then((m) => m.default || m) : theme
  ]));
  const bundledLangs = await Promise.all(langs.map(async (lang) => [
    typeof lang === "string" ? lang : lang.name,
    typeof lang === "string" ? await import(`@shikijs/langs/${lang}`).then((m) => m.default || m) : lang
  ]));
  const highlighter = createShikiHighlighter({
    bundledThemes: Object.fromEntries(bundledThemes),
    // Configure the bundled languages
    bundledLangs: Object.fromEntries(bundledLangs),
    engine: createOnigurumaEngine(import('shiki/wasm')),
    getMdcConfigs: () => Promise.resolve(parserOptions.mdcConfigs)
  });
  highlightPlugin = {
    key,
    instance: rehypeHighlight,
    ...options,
    options: {
      highlighter: async (code, lang, theme, opts) => {
        const result = await highlighter(code, lang, theme, opts);
        const visitTree = {
          type: "element",
          children: result.tree
        };
        if (options.compress) {
          const stylesMap = {};
          visit(
            visitTree,
            (node) => !!node.properties?.style,
            (_node) => {
              const node = _node;
              const style = node.properties.style;
              stylesMap[style] = stylesMap[style] || "s" + hash(style).substring(0, 4);
              node.properties.class = `${node.properties.class || ""} ${stylesMap[style]}`.trim();
              node.properties.style = void 0;
            }
          );
          result.style = Object.entries(stylesMap).map(([style, cls]) => `html pre.shiki code .${cls}, html code.shiki .${cls}{${style}}`).join("") + result.style;
        }
        return result;
      },
      theme: Object.fromEntries(bundledThemes)
    }
  };
  return highlightPlugin;
}
async function createParser(collection, nuxt) {
  const nuxtOptions = nuxt?.options;
  const mdcOptions = nuxtOptions?.mdc || {};
  const { pathMeta = {}, markdown = {}, transformers = [] } = nuxtOptions?.content?.build || {};
  const rehypeHighlightPlugin = markdown.highlight !== false ? await getHighlightPluginInstance(defu$2(markdown.highlight, mdcOptions.highlight, { compress: true })) : void 0;
  let extraTransformers = [];
  if (nuxt?.options?.rootDir) {
    const jiti = createJiti(nuxt.options.rootDir);
    extraTransformers = await Promise.all(transformers.map(async (transformer) => {
      const resolved = resolveAlias(transformer, nuxt?.options?.alias);
      return jiti.import(resolved).then((m) => m.default || m).catch((e) => {
        logger.error(`Failed to load transformer ${transformer}`, e);
        return false;
      });
    })).then((transformers2) => transformers2.filter(Boolean));
  }
  const parserOptions2 = {
    pathMeta,
    markdown: {
      compress: true,
      ...mdcOptions,
      ...markdown,
      rehypePlugins: {
        ...mdcOptions?.rehypePlugins,
        ...markdown?.rehypePlugins,
        // keep highlight plugin last to avoid conflict with other code plugins like `rehype-katex`
        highlight: rehypeHighlightPlugin
      },
      remarkPlugins: {
        "remark-emoji": {},
        ...mdcOptions?.remarkPlugins,
        ...markdown?.remarkPlugins
      },
      highlight: void 0
    }
  };
  return async function parse(file) {
    if (file.path) {
      file.dirname = file.dirname ?? dirname$1(file.path);
      file.extension = file.extension ?? file.path.includes(".") ? "." + file.path.split(".").pop() : void 0;
    }
    if (String(file.body).includes("\r\n")) {
      file.body = file.body.replace(/\r\n/g, "\n");
    }
    const beforeParseCtx = { file, collection, parserOptions: parserOptions2 };
    await nuxt?.callHook?.("content:file:beforeParse", beforeParseCtx);
    const { file: hookedFile } = beforeParseCtx;
    const parsedContent = await transformContent(hookedFile, {
      ...beforeParseCtx.parserOptions,
      transformers: extraTransformers
    });
    const { id, __metadata, ...parsedContentFields } = parsedContent;
    const result = { id };
    const meta = {};
    const collectionKeys = getOrderedSchemaKeys(collection.extendedSchema);
    for (const key of Object.keys(parsedContentFields)) {
      if (collectionKeys.includes(key)) {
        result[key] = parsedContent[key];
      } else {
        meta[key] = parsedContent[key];
      }
    }
    result.meta = meta;
    result.__metadata = __metadata || {};
    if (collectionKeys.includes("rawbody")) {
      result.rawbody = result.rawbody ?? file.body;
    }
    if (collectionKeys.includes("seo")) {
      const seo = result.seo = result.seo || {};
      seo.title = seo.title || result.title;
      seo.description = seo.description || result.description;
    }
    const afterParseCtx = { file: hookedFile, content: result, collection };
    await nuxt?.callHook?.("content:file:afterParse", afterParseCtx);
    return afterParseCtx.content;
  };
}

const compress = (text) => {
  return new Promise((resolve, reject) => gzip(text, (err, buff) => {
    if (err) {
      return reject(err);
    }
    return resolve(buff?.toString("base64"));
  }));
};
function indentLines(str, indent = 2) {
  return str.replace(/ {4}/g, " ".repeat(indent)).split("\n").map((line) => " ".repeat(indent) + line).join("\n");
}
const moduleTemplates = {
  types: "content/types.d.ts",
  preview: "content/preview.mjs",
  manifest: "content/manifest.ts",
  components: "content/components.ts",
  fullCompressedDump: "content/database.compressed.mjs",
  fullRawDump: "content/sql_dump.txt"
};
const contentTypesTemplate = (collections) => ({
  filename: moduleTemplates.types,
  getContents: async ({ options }) => {
    const publicCollections = options.collections.filter((c) => !c.private);
    const pagesCollections = publicCollections.filter((c) => c.type === "page");
    const parentInterface = (c) => c.type === "page" ? "PageCollectionItemBase" : "DataCollectionItemBase";
    return [
      "import type { PageCollectionItemBase, DataCollectionItemBase } from '@nuxt/content'",
      "",
      "declare module '@nuxt/content' {",
      ...await Promise.all(
        publicCollections.map(async (c) => {
          const type = await compile(c.schema, "CLASS").then((code) => code.replace("export interface CLASS", `interface ${pascalCase(c.name)}CollectionItem extends ${parentInterface(c)}`));
          return indentLines(` ${type}`);
        })
      ),
      "",
      "  interface PageCollections {",
      ...pagesCollections.map((c) => indentLines(`${c.name}: ${pascalCase(c.name)}CollectionItem`, 4)),
      "  }",
      "",
      "  interface Collections {",
      ...publicCollections.map((c) => indentLines(`${c.name}: ${pascalCase(c.name)}CollectionItem`, 4)),
      "  }",
      "}",
      ""
    ].join("\n");
  },
  options: {
    collections
  }
});
const fullDatabaseCompressedDumpTemplate = (manifest) => ({
  filename: moduleTemplates.fullCompressedDump,
  getContents: async ({ options }) => {
    const result = [];
    for (const [key, dump] of Object.entries(options.manifest.dump)) {
      if (options.manifest.collections.find((c) => c.name === key)?.private) {
        return "";
      }
      const compressedDump = await compress(JSON.stringify(dump));
      result.push(`export const ${key} = "${compressedDump}"`);
    }
    return result.join("\n");
  },
  write: true,
  options: {
    manifest
  }
});
const fullDatabaseRawDumpTemplate = (manifest) => ({
  filename: moduleTemplates.fullRawDump,
  getContents: ({ options }) => {
    return Object.entries(options.manifest.dump).map(([_key, value]) => {
      return value.join("\n");
    }).join("\n");
  },
  write: true,
  options: {
    manifest
  }
});
const collectionDumpTemplate = (collection, manifest) => ({
  filename: `content/raw/dump.${collection}.sql`,
  getContents: async ({ options }) => {
    return compress(JSON.stringify(options.manifest.dump[collection] || []));
  },
  write: true,
  options: {
    manifest
  }
});
const componentsManifestTemplate = (manifest) => {
  return {
    filename: moduleTemplates.components,
    write: true,
    getContents: ({ app, nuxt, options }) => {
      const componentsMap = app.components.filter((c) => {
        if (c.island) {
          return false;
        }
        if (c.filePath.endsWith(".css")) {
          return false;
        }
        return nuxt.options.dev || options.manifest.components.includes(c.pascalName) || c.global;
      }).reduce((map, c) => {
        map[c.pascalName] = map[c.pascalName] || [
          c.pascalName,
          `${genDynamicImport(isAbsolute(c.filePath) ? "./" + relative(join(nuxt.options.buildDir, "content"), c.filePath).replace(/\b\.(?!vue)\w+$/g, "") : c.filePath.replace(/\b\.(?!vue)\w+$/g, ""), { wrapper: false, singleQuotes: true })}`,
          c.global
        ];
        return map;
      }, {});
      const componentsList = Object.values(componentsMap);
      const globalComponents = componentsList.filter((c) => c[2]).map((c) => c[0]);
      const localComponents = componentsList.filter((c) => !c[2]);
      return [
        ...localComponents.map(([pascalName, type]) => `export const ${pascalName} = () => ${type}`),
        `export const globalComponents: string[] = ${JSON.stringify(globalComponents)}`,
        `export const localComponents: string[] = ${JSON.stringify(localComponents.map((c) => c[0]))}`
      ].join("\n");
    },
    options: {
      manifest
    }
  };
};
const manifestTemplate = (manifest) => ({
  filename: moduleTemplates.manifest,
  getContents: ({ options }) => {
    const collectionsMeta = options.manifest.collections.reduce((acc, collection) => {
      acc[collection.name] = {
        type: collection.type,
        fields: collection.fields
      };
      return acc;
    }, {});
    return [
      `export const checksums = ${JSON.stringify(manifest.checksum, null, 2)}`,
      `export const checksumsStructure = ${JSON.stringify(manifest.checksumStructure, null, 2)}`,
      "",
      `export const tables = ${JSON.stringify(
        Object.fromEntries(manifest.collections.map((c) => [c.name, c.tableName])),
        null,
        2
      )}`,
      "",
      "export default " + JSON.stringify(collectionsMeta, null, 2)
    ].join("\n");
  },
  options: {
    manifest
  },
  write: true
});
const previewTemplate = (collections, gitInfo, schema) => ({
  filename: moduleTemplates.preview,
  getContents: ({ options }) => {
    const collectionsMeta = options.collections.reduce((acc, collection) => {
      const schemaWithCollectionName = {
        ...collection.extendedSchema,
        definitions: {
          [collection.name]: collection.extendedSchema.definitions["__SCHEMA__"]
        }
      };
      acc[collection.name] = {
        name: collection.name,
        pascalName: pascalCase(collection.name),
        tableName: collection.tableName,
        // Remove source from collection meta if it's a remote collection
        source: collection.source?.filter((source) => source.repository ? void 0 : collection.source) || [],
        type: collection.type,
        fields: collection.fields,
        schema: schemaWithCollectionName,
        tableDefinition: generateCollectionTableDefinition(collection)
      };
      return acc;
    }, {});
    const appConfigMeta = {
      properties: schema.properties?.appConfig,
      default: schema.default?.appConfig
    };
    return [
      "export const collections = " + JSON.stringify(collectionsMeta, null, 2),
      "export const gitInfo = " + JSON.stringify(gitInfo, null, 2),
      "export const appConfigSchema = " + JSON.stringify(appConfigMeta, null, 2)
    ].join("\n");
  },
  options: {
    collections,
    gitInfo
  },
  write: true
});

const logger = useLogger("@nuxt/content");
async function startSocketServer(nuxt, options, manifest) {
  const db = await getLocalDatabase(options._localDatabase, { nativeSqlite: options.experimental?.nativeSqlite });
  let websocket;
  let listener;
  const websocketOptions = options.watch || {};
  if (websocketOptions.enabled) {
    nuxt.hook("nitro:init", async (nitro) => {
      websocket = createWebSocket();
      listener = await listen(() => "Nuxt Content", websocketOptions);
      const publicConfig = nitro.options.runtimeConfig.public.content;
      publicConfig.wsUrl = (websocketOptions.publicURL || listener.url).replace("http", "ws");
      listener.server.on("upgrade", websocket.serve);
    });
    nuxt.hook("close", async () => {
      if (websocket) {
        await websocket.close();
      }
      if (listener) {
        await listener.close();
      }
    });
  }
  async function broadcast(collection, key, insertQuery) {
    const removeQuery = `DELETE FROM ${collection.tableName} WHERE id = '${key.replace(/'/g, "''")}';`;
    await db.exec(removeQuery);
    if (insertQuery) {
      await Promise.all(insertQuery.map((query) => db.exec(query)));
    }
    const collectionDump = manifest.dump[collection.name];
    const keyIndex = collectionDump.findIndex((item) => item.includes(`'${key}'`));
    const indexToUpdate = keyIndex !== -1 ? keyIndex : collectionDump.length;
    const itemsToRemove = keyIndex === -1 ? 0 : 1;
    if (insertQuery) {
      collectionDump.splice(indexToUpdate, itemsToRemove, ...insertQuery);
    } else {
      collectionDump.splice(indexToUpdate, itemsToRemove);
    }
    updateTemplates({
      filter: (template) => [
        moduleTemplates.manifest,
        moduleTemplates.fullCompressedDump
        // moduleTemplates.raw,
      ].includes(template.filename)
    });
    websocket?.broadcast({
      key,
      collection: collection.name,
      queries: insertQuery ? [removeQuery, ...insertQuery] : [removeQuery]
    });
  }
  return {
    broadcast
  };
}
async function watchContents(nuxt, options, manifest, socket) {
  const collectionParsers = {};
  const db = await getLocalDatabase(options._localDatabase, { nativeSqlite: options.experimental?.nativeSqlite });
  const collections = manifest.collections;
  const sourceMap = collections.flatMap((c) => {
    if (c.source) {
      return c.source.filter((s) => !s.repository).map((s) => {
        const { fixed } = parseSourceBase(s);
        return { collection: c, source: s, cwd: s.cwd && withTrailingSlash(s.cwd), prefix: s.cwd && withTrailingSlash(join(s.cwd, fixed)) };
      });
    }
    return [];
  }).filter(({ source }) => source.cwd);
  const dirsToWatch = Array.from(new Set(sourceMap.map(({ prefix }) => prefix))).filter(Boolean);
  const watcher = chokidar.watch(dirsToWatch, {
    ignoreInitial: true,
    ignored: (path) => {
      const match = sourceMap.find(({ source, cwd, prefix }) => {
        if (withTrailingSlash(path) === prefix) return true;
        if (prefix && path.startsWith(prefix)) {
          return micromatch.isMatch(
            path.substring(cwd.length),
            "**",
            { ignore: getExcludedSourcePaths(source), dot: true }
          );
        }
        return false;
      });
      return !match;
    }
  });
  watcher.on("add", onChange);
  watcher.on("change", onChange);
  watcher.on("unlink", onRemove);
  async function onChange(pathOrError) {
    if (pathOrError instanceof Error) {
      return;
    }
    let path = resolve(pathOrError);
    const match = sourceMap.find(({ source, cwd }) => {
      if (cwd && path.startsWith(cwd)) {
        return micromatch.isMatch(path.substring(cwd.length), source.include, { ignore: source.exclude || [], dot: true });
      }
      return false;
    });
    if (match) {
      const { collection, source, cwd } = match;
      path = path.substring(cwd.length);
      logger.info(`File \`${path}\` changed on \`${collection.name}\` collection`);
      const { fixed } = parseSourceBase(source);
      const filePath = path.substring(fixed.length);
      const keyInCollection = join(collection.name, source?.prefix || "", filePath);
      const fullPath = join(cwd, path);
      const content = await readFile(fullPath, "utf8");
      const checksum = getContentChecksum(content);
      const localCache = await db.fetchDevelopmentCacheForKey(keyInCollection);
      let parsedContent = localCache?.value || "";
      if (!localCache || localCache?.checksum !== checksum) {
        if (!collectionParsers[collection.name]) {
          collectionParsers[collection.name] = await createParser(collection, nuxt);
        }
        const parser = collectionParsers[collection.name];
        parsedContent = await parser({
          id: keyInCollection,
          body: content,
          path: fullPath
        }).then((result) => JSON.stringify(result));
        db.insertDevelopmentCache(keyInCollection, checksum, parsedContent);
      }
      const { queries: insertQuery } = generateCollectionInsert(collection, JSON.parse(parsedContent));
      await socket.broadcast(collection, keyInCollection, insertQuery);
    }
  }
  async function onRemove(pathOrError) {
    if (pathOrError instanceof Error) {
      return;
    }
    let path = resolve(pathOrError);
    const match = sourceMap.find(({ source, cwd }) => {
      if (cwd && path.startsWith(cwd)) {
        return micromatch.isMatch(path.substring(cwd.length), source.include, { ignore: source.exclude || [], dot: true });
      }
      return false;
    });
    if (match) {
      const { collection, source, cwd } = match;
      path = path.substring(cwd.length);
      logger.info(`File \`${path}\` removed from \`${collection.name}\` collection`);
      const { fixed } = parseSourceBase(source);
      const filePath = path.substring(fixed.length);
      const keyInCollection = join(collection.name, source?.prefix || "", filePath);
      await db.deleteDevelopmentCache(keyInCollection);
      await socket.broadcast(collection, keyInCollection);
    }
  }
  nuxt.hook("close", async () => {
    if (watcher) {
      watcher.removeAllListeners();
      watcher.close();
      db.close();
    }
  });
}
function watchComponents(nuxt) {
  const contentDir = join(nuxt.options.rootDir, "content");
  const componentsTemplatePath = join(nuxt.options.buildDir, "content/components.ts");
  nuxt.options.vite.server ||= {};
  nuxt.options.vite.server.watch ||= {};
  nuxt.options.vite.server.watch.ignored = (file) => {
    if (file.startsWith(contentDir)) {
      return true;
    }
    return file !== componentsTemplatePath && isIgnored(file);
  };
  let componentDirs = [];
  nuxt.hook("components:dirs", (allDirs) => {
    componentDirs = allDirs.map((dir) => typeof dir === "string" ? dir : dir.path).filter(Boolean);
  });
  nuxt.hook("builder:watch", async (event, relativePath) => {
    if (!["add", "unlink"].includes(event)) {
      return;
    }
    const path = resolve(nuxt.options.srcDir, relativePath);
    if (componentDirs.some((dir) => path.startsWith(dir + "/"))) {
      await updateTemplates({
        filter: (template) => [moduleTemplates.components].includes(template.filename)
      });
    }
  });
  addVitePlugin({
    name: "reload",
    configureServer(server) {
      server.watcher.on("change", (file) => {
        if (file === componentsTemplatePath) {
          server.ws.send({
            type: "full-reload"
          });
        }
      });
    }
  });
}
function createWebSocket() {
  const wss = new WebSocketServer({ noServer: true });
  const serve = (req, socket = req.socket, head) => wss.handleUpgrade(req, socket, head, (client) => {
    wss.emit("connection", client, req);
  });
  const broadcast = (data) => {
    const message = JSON.stringify(data);
    for (const client of wss.clients) {
      try {
        client.send(message);
      } catch (err) {
        console.log(err);
      }
    }
  };
  return {
    serve,
    broadcast,
    close: () => {
      wss.clients.forEach((client) => client.close());
      return new Promise((resolve2) => wss.close(resolve2));
    }
  };
}
function getContentChecksum(content) {
  return crypto.createHash("md5").update(content, "utf8").digest("hex");
}
function* chunks(arr, size) {
  for (let i = 0; i < arr.length; i += size) {
    yield arr.slice(i, i + size);
  }
}

function getExcludedSourcePaths(source) {
  return [
    ...source.exclude || [],
    // Ignore OS files
    "**/.DS_Store"
  ];
}
function defineLocalSource(source) {
  if (source.include.startsWith("./") || source.include.startsWith("../")) {
    logger.warn("Collection source should not start with `./` or `../`.");
    source.include = source.include.replace(/^(\.\/|\.\.\/|\/)*/, "");
  }
  const { fixed } = parseSourceBase(source);
  const resolvedSource = {
    _resolved: true,
    prefix: withoutTrailingSlash(withLeadingSlash(fixed)),
    prepare: async ({ rootDir }) => {
      resolvedSource.cwd = source.cwd ? String(normalize(source.cwd)).replace(/^~~\//, rootDir) : join(rootDir, "content");
    },
    getKeys: async () => {
      const _keys = await glob(source.include, { cwd: resolvedSource.cwd, ignore: getExcludedSourcePaths(source), dot: true, expandDirectories: false }).catch(() => []);
      return _keys.map((key) => key.substring(fixed.length));
    },
    getItem: async (key) => {
      const fullPath = join(resolvedSource.cwd, fixed, key);
      const content = await readFile(fullPath, "utf8");
      return content;
    },
    ...source,
    include: source.include,
    cwd: ""
  };
  return resolvedSource;
}
function defineGitHubSource(source) {
  const resolvedSource = defineLocalSource(source);
  resolvedSource.prepare = async ({ rootDir }) => {
    const repository = source?.repository && parseGitHubUrl(source.repository);
    if (repository) {
      const { org, repo, branch } = repository;
      resolvedSource.cwd = join(rootDir, ".data", "content", `github-${org}-${repo}-${branch}`);
      let headers = {};
      if (resolvedSource.authToken) {
        headers = { Authorization: `Bearer ${resolvedSource.authToken}` };
      }
      const url = headers.Authorization ? `https://api.github.com/repos/${org}/${repo}/tarball/${branch}` : `https://github.com/${org}/${repo}/archive/refs/heads/${branch}.tar.gz`;
      await downloadRepository(url, resolvedSource.cwd, { headers });
    }
  };
  return resolvedSource;
}
function defineBitbucketSource(source) {
  const resolvedSource = defineLocalSource(source);
  resolvedSource.prepare = async ({ rootDir }) => {
    const repository = source?.repository && parseBitBucketUrl(source.repository);
    if (repository) {
      const { org, repo, branch } = repository;
      resolvedSource.cwd = join(
        rootDir,
        ".data",
        "content",
        `bitbucket-${org}-${repo}-${branch}`
      );
      let headers = {};
      if (resolvedSource.authBasic) {
        const credentials = `${resolvedSource.authBasic.username}:${resolvedSource.authBasic.password}`;
        const encodedCredentials = btoa(credentials);
        headers = {
          Authorization: `Basic ${encodedCredentials}`
        };
      }
      const url = `https://bitbucket.org/${org}/${repo}/get/${branch}.tar.gz`;
      await downloadRepository(url, resolvedSource.cwd, { headers });
    }
  };
  return resolvedSource;
}
function parseSourceBase(source) {
  const [fixPart, ...rest] = source.include.includes("*") ? source.include.split("*") : ["", source.include];
  return {
    fixed: fixPart || "",
    dynamic: "*" + rest.join("*")
  };
}

const infoStandardSchema = {
  $ref: "#/definitions/info",
  definitions: {
    info: {
      type: "object",
      properties: {
        id: {
          type: "string"
        },
        version: {
          type: "string"
        },
        structureVersion: {
          type: "string"
        },
        ready: {
          type: "boolean"
        }
      },
      required: [
        "id",
        "version",
        "structureVersion",
        "ready"
      ],
      additionalProperties: false
    }
  },
  $schema: "http://json-schema.org/draft-07/schema#"
};
const emptyStandardSchema = {
  $schema: "http://json-schema.org/draft-07/schema#",
  $ref: "#/definitions/__SCHEMA__",
  definitions: {
    __SCHEMA__: {
      type: "object",
      properties: {},
      required: [],
      additionalProperties: false
    }
  }
};
const metaStandardSchema = {
  $schema: "http://json-schema.org/draft-07/schema#",
  $ref: "#/definitions/__SCHEMA__",
  definitions: {
    __SCHEMA__: {
      type: "object",
      properties: {
        id: {
          type: "string"
        },
        stem: {
          type: "string"
        },
        extension: {
          type: "string",
          enum: [
            "md",
            "yaml",
            "yml",
            "json",
            "csv",
            "xml"
          ]
        },
        meta: {
          type: "object",
          additionalProperties: {}
        }
      },
      required: [
        "id",
        "stem",
        "extension",
        "meta"
      ],
      additionalProperties: false
    }
  }
};
const pageStandardSchema = {
  $schema: "http://json-schema.org/draft-07/schema#",
  $ref: "#/definitions/__SCHEMA__",
  definitions: {
    __SCHEMA__: {
      type: "object",
      properties: {
        path: {
          type: "string"
        },
        title: {
          type: "string"
        },
        description: {
          type: "string"
        },
        seo: {
          allOf: [
            {
              type: "object",
              properties: {
                title: {
                  type: "string"
                },
                description: {
                  type: "string"
                }
              }
            },
            {
              type: "object",
              additionalProperties: {}
            }
          ],
          default: {}
        },
        body: {
          type: "object",
          properties: {
            type: {
              type: "string"
            },
            children: {},
            toc: {}
          },
          required: [
            "type"
          ],
          additionalProperties: false
        },
        navigation: {
          anyOf: [
            {
              type: "boolean"
            },
            {
              type: "object",
              properties: {
                title: {
                  type: "string"
                },
                description: {
                  type: "string"
                },
                icon: {
                  type: "string",
                  $content: {
                    editor: {
                      input: "icon"
                    }
                  }
                }
              },
              required: [
                "title",
                "description",
                "icon"
              ],
              additionalProperties: false
            }
          ],
          default: true
        }
      },
      required: [
        "path",
        "title",
        "description",
        "body"
      ],
      additionalProperties: false
    }
  }
};

function property(input) {
  const $content = {};
  const attachContent = (schema) => {
    if (!schema) return;
    const vendor = detectSchemaVendor(schema);
    switch (vendor) {
      case "valibot":
        schema.$content = $content;
        return;
      case "zod4":
        schema.def.$content = $content;
        return;
      case "zod3":
        schema._def.$content = $content;
        return;
    }
  };
  attachContent(input);
  const createProxy = (target) => new Proxy(target, {
    get(_target, prop, receiver) {
      if (prop === "editor") {
        return (opts) => {
          const current = $content.editor || {};
          $content.editor = { ...current, ...opts };
          return receiver;
        };
      }
      if (prop === "inherit") {
        return (componentPath) => {
          $content.inherit = componentPath;
          return receiver;
        };
      }
      const value = Reflect.get(_target, prop, receiver);
      if (typeof value === "function") {
        return (...args) => value.apply(_target, args);
      }
      return value;
    }
  });
  return createProxy(input);
}
function mergeStandardSchema(s1, s2) {
  return {
    $schema: s1.$schema,
    $ref: s1.$ref,
    definitions: Object.fromEntries(
      Object.entries(s1.definitions).map(([key, def1]) => {
        const def2 = s2.definitions[key];
        if (!def2) return [key, def1];
        return [key, {
          ...def1,
          properties: { ...def1.properties, ...def2.properties },
          required: [.../* @__PURE__ */ new Set([...def1.required, ...def2.required || []])]
        }];
      })
    )
  };
}
function detectSchemaVendor(schema) {
  if (schema["~standard"]?.vendor === "valibot") {
    return "valibot";
  }
  if (schema["~standard"]?.vendor === "zod") {
    return schema.def ? "zod4" : "zod3";
  }
  return "unknown";
}
function replaceComponentSchemas(property2) {
  if (property2.type === "array") {
    property2.items = replaceComponentSchemas(property2.items);
  }
  if (property2.type !== "object") {
    return property2;
  }
  const $content = property2.$content;
  if ($content?.inherit) {
    const nuxt = useNuxt();
    let path = String($content?.inherit);
    try {
      path = resolveModule(path, { paths: [nuxt.options.rootDir] });
    } catch {
    }
    const meta = getComponentMeta(path, { rootDir: nuxt.options.rootDir, cache: true });
    return propsToJsonSchema(meta.props);
  }
  if (property2.properties) {
    Object.entries(property2.properties).forEach(([key, value]) => {
      property2.properties[key] = replaceComponentSchemas(value);
    });
  }
  return property2;
}

function getTableName(name) {
  return `_content_${name}`;
}
function defineCollection(collection) {
  let standardSchema = emptyStandardSchema;
  if (collection.schema) {
    const schemaCtx = nuxtContentContext$1().get(detectSchemaVendor(collection.schema));
    standardSchema = schemaCtx.toJSONSchema(collection.schema, "__SCHEMA__");
  }
  standardSchema.definitions.__SCHEMA__ = replaceComponentSchemas(standardSchema.definitions.__SCHEMA__);
  let extendedSchema = standardSchema;
  if (collection.type === "page") {
    extendedSchema = mergeStandardSchema(pageStandardSchema, extendedSchema);
  }
  extendedSchema = mergeStandardSchema(metaStandardSchema, extendedSchema);
  return {
    type: collection.type,
    source: resolveSource(collection.source),
    schema: standardSchema,
    extendedSchema,
    fields: getCollectionFieldsTypes(extendedSchema)
  };
}
function defineCollectionSource(source) {
  const resolvedSource = resolveSource({ ...source, cwd: "", include: "" })?.[0];
  if (!resolvedSource) {
    throw new Error("Invalid collection source");
  }
  return {
    _custom: true,
    ...resolvedSource
  };
}
function resolveCollection(name, collection) {
  if (/^[a-z_]\w*$/i.test(name) === false) {
    logger.warn([
      `Collection name "${name}" is invalid. Collection names must be valid JavaScript identifiers. This collection will be ignored.`
    ].join("\n"));
    return void 0;
  }
  return {
    ...collection,
    name,
    type: collection.type || "page",
    tableName: getTableName(name),
    private: name === "info"
  };
}
function resolveCollections(collections) {
  collections.info = {
    type: "data",
    source: void 0,
    schema: infoStandardSchema,
    extendedSchema: infoStandardSchema,
    fields: {}
  };
  return Object.entries(collections).map(([name, collection]) => resolveCollection(name, collection)).filter(Boolean);
}
function resolveSource(source) {
  if (!source) {
    return void 0;
  }
  if (typeof source === "string") {
    return [defineLocalSource({ include: source })];
  }
  const sources = Array.isArray(source) ? source : [source];
  return sources.map((source2) => {
    if (source2._resolved) {
      return source2;
    }
    if (source2.repository) {
      if (source2.repository.startsWith("https://bitbucket.org/")) {
        return defineBitbucketSource(source2);
      }
      return defineGitHubSource(source2);
    }
    return defineLocalSource(source2);
  });
}
const MAX_SQL_QUERY_SIZE = 1e5;
const SLICE_SIZE = 7e4;
function generateCollectionInsert(collection, data) {
  const fields = [];
  const values = [];
  const sortedKeys = getOrderedSchemaKeys(collection.extendedSchema);
  sortedKeys.forEach((key) => {
    const property = describeProperty(collection.extendedSchema, key);
    const defaultValue = "default" in property ? property.default : "NULL";
    const valueToInsert = typeof data[key] === "undefined" || String(data[key]) === "null" ? defaultValue : data[key];
    fields.push(key);
    if (valueToInsert === "NULL") {
      values.push(valueToInsert);
      return;
    }
    if (property?.json) {
      values.push(`'${JSON.stringify(valueToInsert).replace(/'/g, "''")}'`);
    } else if (property?.sqlType === "BOOLEAN") {
      values.push(!!valueToInsert);
    } else if (property?.sqlType === "INT") {
      values.push(Number(valueToInsert));
    } else if (property?.sqlType === "DATE") {
      values.push(`'${new Date(valueToInsert).toISOString()}'`);
    } else if (property?.enum) {
      values.push(`'${String(valueToInsert).replace(/\n/g, "\\n").replace(/'/g, "''")}'`);
    } else if ((property?.sqlType || "").match(/^(VARCHAR|TEXT)/)) {
      values.push(`'${String(valueToInsert).replace(/'/g, "''")}'`);
    } else {
      values.push(String(valueToInsert));
    }
  });
  const valuesHash = hash(values);
  values.push(`'${valuesHash}'`);
  let index = 0;
  const sql = `INSERT INTO ${collection.tableName} VALUES (${"?, ".repeat(values.length).slice(0, -2)});`.replace(/\?/g, () => values[index++]);
  if (sql.length < MAX_SQL_QUERY_SIZE) {
    return {
      queries: [sql],
      hash: valuesHash
    };
  }
  const biggestColumn = [...values].sort((a, b) => String(b).length - String(a).length)[0];
  const bigColumnIndex = values.indexOf(biggestColumn);
  const bigColumnName = fields[bigColumnIndex];
  if (typeof biggestColumn === "string") {
    let sliceIndex = SLICE_SIZE;
    values[bigColumnIndex] = `${biggestColumn.slice(0, sliceIndex)}'`;
    index = 0;
    const bigValueSliceWithHash = [...values.slice(0, -1), `'${valuesHash}-${sliceIndex}'`];
    const SQLQueries = [
      `INSERT INTO ${collection.tableName} VALUES (${"?, ".repeat(bigValueSliceWithHash.length).slice(0, -2)});`.replace(/\?/g, () => bigValueSliceWithHash[index++])
    ];
    while (sliceIndex < biggestColumn.length) {
      const prevSliceIndex = sliceIndex;
      sliceIndex += SLICE_SIZE;
      const isLastSlice = sliceIndex > biggestColumn.length;
      const newSlice = `'${biggestColumn.slice(prevSliceIndex, sliceIndex)}` + (!isLastSlice ? "'" : "");
      const sliceHash = isLastSlice ? valuesHash : `${valuesHash}-${sliceIndex}`;
      SQLQueries.push([
        "UPDATE",
        collection.tableName,
        `SET ${bigColumnName} = CONCAT(${bigColumnName}, ${newSlice}), "__hash__" = '${sliceHash}'`,
        "WHERE",
        `id = ${values[0]} AND "__hash__" = '${valuesHash}-${prevSliceIndex}';`
      ].join(" "));
    }
    return { queries: SQLQueries, hash: valuesHash };
  }
  return {
    queries: [sql],
    hash: valuesHash
  };
}
function generateCollectionTableDefinition(collection, opts = {}) {
  const sortedKeys = getOrderedSchemaKeys(collection.extendedSchema);
  const sqlFields = sortedKeys.map((key) => {
    if (key === "id") return `${key} TEXT PRIMARY KEY`;
    const property = describeProperty(collection.extendedSchema, key);
    let sqlType = property?.sqlType;
    if (!sqlType) throw new Error(`Unsupported Zod type: ${property?.type}`);
    if (property.sqlType === "VARCHAR" && property.maxLength) {
      sqlType += `(${property.maxLength})`;
    }
    const constraints = [
      property?.nullable ? " NULL" : ""
    ];
    if ("default" in property) {
      let defaultValue = typeof property.default === "string" ? wrapWithSingleQuote(property.default) : property.default;
      if (!(defaultValue instanceof Date) && typeof defaultValue === "object") {
        defaultValue = wrapWithSingleQuote(JSON.stringify(defaultValue));
      }
      constraints.push(`DEFAULT ${defaultValue}`);
    }
    return `"${key}" ${sqlType}${constraints.join(" ")}`;
  });
  sqlFields.push('"__hash__" TEXT UNIQUE');
  let definition = `CREATE TABLE IF NOT EXISTS ${collection.tableName} (${sqlFields.join(", ")});`;
  if (opts.drop) {
    definition = `DROP TABLE IF EXISTS ${collection.tableName};
${definition}`;
  }
  return definition;
}
function wrapWithSingleQuote(value) {
  if (value.startsWith("`") && value.endsWith("`")) {
    value = value.slice(1, -1);
  }
  if (value.startsWith("'") && value.endsWith("'")) {
    return value;
  }
  return `'${value}'`;
}

const defaultConfig = {
  collections: {
    content: defineCollection({
      type: "page",
      source: "**/*"
    })
  }
};
const defineContentConfig = createDefineConfig();
async function loadContentConfig(nuxt) {
  const loader = nuxt.options.dev ? (opts) => watchConfig({
    ...opts,
    onWatch: (e) => {
      logger.info(relative(nuxt.options.rootDir, e.path) + " " + e.type + ", restarting the Nuxt server...");
      nuxt.hooks.callHook("restart", { hard: true });
    }
  }) : loadConfig;
  globalThis.defineContentConfig = (c) => c;
  const layers = [...nuxt.options._layers].reverse();
  const contentConfigs = await Promise.all(
    layers.map(
      (layer) => loader({ name: "content", cwd: layer.config.rootDir, defaultConfig: { collections: {} } })
    )
  );
  delete globalThis.defineContentConfig;
  if (nuxt.options.dev) {
    nuxt.hook("close", () => Promise.all(contentConfigs.map((c) => c.unwatch())).then(() => {
    }));
  }
  const collectionsConfig = contentConfigs.reduce((acc, curr) => {
    const layerCollections = curr.config?.collections || {};
    const cwd = curr.cwd;
    Object.entries(layerCollections).forEach(([name, collection]) => {
      collection.__rootDir = cwd;
      acc[name] = collection;
    });
    return acc;
  }, {});
  const hasNoCollections = Object.keys(collectionsConfig || {}).length === 0;
  if (hasNoCollections) {
    logger.warn("No content configuration found, falling back to default collection. In order to have full control over your collections, create the config file in project root. See: https://content.nuxt.com/docs/getting-started/installation");
  }
  const collections = resolveCollections(hasNoCollections ? defaultConfig.collections : collectionsConfig);
  return { collections };
}

async function installMDCModule(contentOptions, nuxt) {
  const options = nuxt.options;
  const highlight = contentOptions.build?.markdown?.highlight;
  options.mdc = defu$2({
    highlight: highlight !== false ? { ...highlight, noApiRoute: options.mdc?.highlight?.noApiRoute ?? true } : highlight,
    components: {
      prose: true,
      map: contentOptions.renderer.alias
    },
    headings: {
      anchorLinks: contentOptions.renderer.anchorLinks
    },
    remarkPlugins: contentOptions.build?.markdown?.remarkPlugins,
    rehypePlugins: contentOptions.build?.markdown?.rehypePlugins
  }, options.mdc);
  await nuxt.hook("mdc:configSources", async (mdcConfigs) => {
    if (mdcConfigs.length) {
      const jiti = createJiti(nuxt.options.rootDir);
      const configs = await Promise.all(mdcConfigs.map((path) => jiti.import(path).then((m) => m.default || m)));
      setParserOptions({
        mdcConfigs: configs
      });
    }
  });
  await installModule("@nuxtjs/mdc");
  extendViteConfig((config) => {
    config.optimizeDeps ||= {};
    config.optimizeDeps.include ||= [];
    config.optimizeDeps.include.push("@nuxt/content > slugify");
    config.optimizeDeps.include = config.optimizeDeps.include.map((id) => id.replace(/^@nuxtjs\/mdc > /, "@nuxt/content > @nuxtjs/mdc > "));
  });
}

function definePreset(preset) {
  const _preset = {
    ...preset,
    setup: async (options, nuxt) => {
      if (preset.parent) {
        await preset.parent.setup?.(options, nuxt);
      }
      await preset.setup?.(options, nuxt);
    },
    setupNitro: async (nitroConfig, opts) => {
      if (preset.parent) {
        await preset.parent.setupNitro?.(nitroConfig, opts);
      }
      await preset.setupNitro?.(nitroConfig, opts);
    }
  };
  return _preset;
}

const node = definePreset({
  name: "node",
  setupNitro(nitroConfig, { manifest, resolver }) {
    nitroConfig.publicAssets ||= [];
    nitroConfig.alias = nitroConfig.alias || {};
    nitroConfig.handlers ||= [];
    nitroConfig.alias["#content/dump"] = addTemplate(fullDatabaseCompressedDumpTemplate(manifest)).dst;
    nitroConfig.handlers.push({
      route: "/__nuxt_content/:collection/sql_dump.txt",
      handler: resolver.resolve("./runtime/presets/node/database-handler")
    });
  }
});

const awsAmplify = definePreset({
  name: "aws-amplify",
  parent: node,
  async setup(options, nuxt) {
    options.database ||= { type: "sqlite", filename: "/tmp/contents.sqlite" };
    Object.keys(nuxt.options.routeRules || {}).forEach((route) => {
      if (route.startsWith("/__nuxt_content/") && route.endsWith("/sql_dump.txt")) {
        nuxt.options.routeRules[route].prerender = false;
      }
    });
    try {
      await import('sqlite3');
      options.experimental ||= {};
      options.experimental.sqliteConnector = "sqlite3";
    } catch {
      logger.error("Nuxt Content requires `sqlite3` module to work in AWS Amplify environment. Please run `npm install sqlite3` to install it and try again.");
      process.exit(1);
    }
  },
  async setupNitro(nitroConfig) {
    const database = nitroConfig.runtimeConfig?.content?.database;
    if (database?.type === "sqlite" && !database?.filename?.startsWith("/tmp")) {
      logger.warn("Deploying sqlite database to AWS Amplify is possible only in `/tmp` directory. Using `/tmp/contents.sqlite` instead.");
      database.filename = "/tmp/contents.sqlite";
    }
  }
});

const cloudflare = definePreset({
  name: "cloudflare",
  async setupNitro(nitroConfig, { manifest, resolver }) {
    if (nitroConfig.runtimeConfig?.content?.database?.type === "sqlite") {
      logger.warn("Deploying to Cloudflare requires using D1 database, switching to D1 database with binding `DB`.");
      nitroConfig.runtimeConfig.content.database = { type: "d1", bindingName: "DB" };
    }
    nitroConfig.publicAssets ||= [];
    nitroConfig.alias = nitroConfig.alias || {};
    nitroConfig.handlers ||= [];
    manifest.collections.map(async (collection) => {
      if (!collection.private) {
        addTemplate(collectionDumpTemplate(collection.name, manifest));
      }
    });
    nitroConfig.publicAssets.push({ dir: join(nitroConfig.buildDir, "content", "raw"), maxAge: 60 });
    nitroConfig.handlers.push({
      route: "/__nuxt_content/:collection/sql_dump.txt",
      handler: resolver.resolve("./runtime/presets/cloudflare/database-handler")
    });
  }
});

const netlify = definePreset({
  name: "netlify",
  parent: node,
  async setup(options) {
    options.database ||= { type: "sqlite", filename: "/tmp/contents.sqlite" };
  },
  async setupNitro(nitroConfig) {
    const database = nitroConfig.runtimeConfig?.content?.database;
    if (database?.type === "sqlite" && !database?.filename?.startsWith("/tmp")) {
      logger.warn("Deploying sqlite database to Netlify is possible only in `/tmp` directory. Using `/tmp/contents.sqlite` instead.");
      database.filename = "/tmp/contents.sqlite";
    }
  }
});

const nuxthub = definePreset({
  name: "nuxthub",
  async setup(options, nuxt) {
    const indexOfNuxtHub = nuxt.options.modules.indexOf("@nuxthub/core");
    const indexOfContentModule = nuxt.options.modules.indexOf("@nuxt/content");
    if (!nuxt.options.hub?.database && indexOfNuxtHub < indexOfContentModule) {
      logger.warn("NuxtHub database is not enabled. Please enable it in your NuxtHub configuration. It is recommended to register `@nuxt/content` before `@nuxthub/core`, so that `@nuxt/content` can automatically configure the database if needed.");
    }
    const nuxthubOptions = nuxt.options.hub ||= {};
    nuxthubOptions.database = true;
    options.database ||= { type: "d1", bindingName: "DB" };
  },
  async setupNitro(nitroConfig, options) {
    if (nitroConfig.runtimeConfig?.content?.database?.type === "sqlite") {
      logger.warn("Deploying to NuxtHub requires using D1 database, switching to D1 database with binding `DB`.");
      nitroConfig.runtimeConfig.content.database = { type: "d1", bindingName: "DB" };
    }
    await cloudflare.setupNitro(nitroConfig, options);
    if (nitroConfig.dev === false) {
      await mkdir(resolve(nitroConfig.rootDir, ".data/hub/database/queries"), { recursive: true });
      let i = 1;
      let dump = "DROP TABLE IF EXISTS _content_info;";
      const dumpFiles = [];
      Object.values(options.manifest.dump).forEach((value) => {
        value.forEach((line) => {
          if (dump.length + line.length < 1e6) {
            dump += "\n" + line;
          } else {
            dumpFiles.push({ file: `content-database-${String(i).padStart(3, "0")}.sql`, content: dump.trim() });
            dump = line;
            i += 1;
          }
        });
      });
      if (dump.length > 0) {
        dumpFiles.push({ file: `content-database-${String(i).padStart(3, "0")}.sql`, content: dump.trim() });
      }
      for (const dumpFile of dumpFiles) {
        await writeFile(resolve(nitroConfig.rootDir, ".data/hub/database/queries", dumpFile.file), dumpFile.content);
      }
      nitroConfig.runtimeConfig ||= {};
      nitroConfig.runtimeConfig.content ||= {};
      nitroConfig.runtimeConfig.content.integrityCheck = false;
    }
  }
});

const vercel = definePreset({
  name: "vercel",
  parent: node,
  async setup(options) {
    options.database ||= { type: "sqlite", filename: "/tmp/contents.sqlite" };
  },
  async setupNitro(nitroConfig) {
    const database = nitroConfig.runtimeConfig?.content?.database;
    if (database?.type === "sqlite" && !database?.filename?.startsWith("/tmp")) {
      logger.warn("Deploying sqlite database to Vercel is possible only in `/tmp` directory. Using `/tmp/contents.sqlite` instead.");
      database.filename = "/tmp/contents.sqlite";
    }
  }
});

function findPreset(nuxt) {
  const preset = nuxt.options.nitro.preset?.replace(/_/g, "-");
  if (hasNuxtModule("@nuxthub/core", nuxt)) {
    return nuxthub;
  }
  if (["cloudflare-pages", "cloudflare-module", "cloudflare-durable"].includes(preset || "")) {
    return cloudflare;
  }
  if (preset === "netlify-legacy" || process.env.NETLIFY === "true") {
    return netlify;
  }
  if (preset === "vercel" || process.env.VERCEL === "1") {
    return vercel;
  }
  if (preset === "aws-amplify" || typeof process.env.AWS_AMPLIFY_DEPLOYMENT_ID !== "undefined") {
    return awsAmplify;
  }
  return node;
}

async function setupPreview(options, nuxt, resolver, manifest) {
  const previewOptions = options.preview;
  const { resolve } = resolver;
  const api = process.env.NUXT_CONTENT_PREVIEW_API || previewOptions.api;
  const iframeMessagingAllowedOrigins = process.env.PREVIEW_ALLOWED_ORIGINS;
  const gitInfo = previewOptions.gitInfo || await getLocalGitInfo(nuxt.options.rootDir) || getGitEnv() || {};
  nuxt.options.runtimeConfig.public.preview = { api, iframeMessagingAllowedOrigins };
  if (process.env.NUXT_CONTENT_PREVIEW_STAGING_API) {
    nuxt.options.runtimeConfig.public.preview.stagingApi = process.env.NUXT_CONTENT_PREVIEW_STAGING_API;
  }
  nuxt.hook("schema:resolved", (schema) => {
    const template = addTemplate(previewTemplate(manifest.collections, gitInfo, schema)).dst;
    nuxt.options.nitro.alias ||= {};
    nuxt.options.nitro.alias["#content/preview"] = template;
    nuxt.options.alias["#content/preview"] = template;
  });
  addPlugin(resolver.resolve("./runtime/plugins/preview.client"));
  addComponent({ name: "ContentPreviewMode", filePath: resolver.resolve("./runtime/components/ContentPreviewMode.vue") });
  addServerHandler({
    method: "get",
    route: "/__preview.json",
    handler: resolve("./runtime/api/preview")
  });
  addPrerenderRoutes("/__preview.json");
  await installModule("nuxt-component-meta", {
    globalsOnly: true,
    include: manifest.components
  });
}
function shouldEnablePreview(nuxt, options) {
  if (process.env.NUXT_CONTENT_PREVIEW_API || options.preview?.api) {
    if (nuxt.options.dev === true && !options.preview?.dev) {
      return false;
    }
    return true;
  }
  return false;
}

const defu = createDefu((obj, key, value) => {
  if (Array.isArray(obj[key]) && Array.isArray(value)) {
    obj[key] = value;
    return true;
  }
});
z$1.ZodType.prototype.editor = function(options) {
  this._def.editor = { ...this._def.editor, ...options };
  return this;
};
const z = z$1;
function toJSONSchema(_schema, name) {
  const schema = _schema;
  const jsonSchema = zodToJsonSchema(schema, { name, $refStrategy: "none" });
  const jsonSchemaWithEditorMeta = zodToJsonSchema(
    schema,
    {
      name,
      $refStrategy: "none",
      override: (_def) => {
        const def = _def;
        if (def.editor) {
          return {
            $content: {
              editor: def.editor
            }
          };
        }
        if (def.$content) {
          return {
            $content: def.$content
          };
        }
        return ignoreOverride;
      }
    }
  );
  return defu(jsonSchema, jsonSchemaWithEditorMeta);
}

const zod3 = {
  __proto__: null,
  toJSONSchema: toJSONSchema,
  z: z
};

const module = defineNuxtModule({
  meta: {
    name: "@nuxt/content",
    configKey: "content",
    version,
    docs: "https://content.nuxt.com"
  },
  defaults: {
    _localDatabase: {
      type: "sqlite",
      filename: ".data/content/contents.sqlite"
    },
    preview: {},
    watch: {
      enabled: true,
      port: {
        port: 4e3,
        portRange: [4e3, 4040]
      },
      hostname: "localhost",
      showURL: false
    },
    renderer: {
      alias: {},
      anchorLinks: {
        h2: true,
        h3: true,
        h4: true
      }
    },
    build: {
      pathMeta: {},
      markdown: {},
      yaml: {},
      csv: {
        delimiter: ",",
        json: true
      }
    },
    experimental: {
      nativeSqlite: false
    }
  },
  async setup(options, nuxt) {
    const resolver = createResolver(import.meta.url);
    const manifest = {
      checksumStructure: {},
      checksum: {},
      dump: {},
      components: [],
      collections: []
    };
    await initiateValidatorsContext();
    const { collections } = await loadContentConfig(nuxt);
    manifest.collections = collections;
    nuxt.options.vite.optimizeDeps ||= {};
    nuxt.options.vite.optimizeDeps.exclude ||= [];
    nuxt.options.vite.optimizeDeps.exclude.push("@sqlite.org/sqlite-wasm");
    nuxt.options.ignore = [...nuxt.options.ignore || [], "content/**"];
    addImports([
      { name: "queryCollection", from: resolver.resolve("./runtime/app") },
      { name: "queryCollectionSearchSections", from: resolver.resolve("./runtime/app") },
      { name: "queryCollectionNavigation", from: resolver.resolve("./runtime/app") },
      { name: "queryCollectionItemSurroundings", from: resolver.resolve("./runtime/app") }
    ]);
    addServerImports([
      { name: "queryCollection", from: resolver.resolve("./runtime/nitro") },
      { name: "queryCollectionSearchSections", from: resolver.resolve("./runtime/nitro") },
      { name: "queryCollectionNavigation", from: resolver.resolve("./runtime/nitro") },
      { name: "queryCollectionItemSurroundings", from: resolver.resolve("./runtime/nitro") }
    ]);
    addComponent({ name: "ContentRenderer", filePath: resolver.resolve("./runtime/components/ContentRenderer.vue") });
    nuxt.options.nitro.alias = nuxt.options.nitro.alias || {};
    addTemplate(fullDatabaseRawDumpTemplate(manifest));
    nuxt.options.alias["#content/components"] = addTemplate(componentsManifestTemplate(manifest)).dst;
    nuxt.options.alias["#content/manifest"] = addTemplate(manifestTemplate(manifest)).dst;
    const typesTemplateDst = addTypeTemplate(contentTypesTemplate(manifest.collections)).dst;
    nuxt.options.nitro.typescript ||= {};
    nuxt.options.nitro.typescript.tsConfig = defu$1(nuxt.options.nitro.typescript.tsConfig, {
      include: [typesTemplateDst]
    });
    const _layers = [...nuxt.options._layers].reverse();
    for (const layer of _layers) {
      const path = resolver.resolve(layer.config.srcDir, "components/content");
      const dirStat = await stat(path).catch(() => null);
      if (dirStat && dirStat.isDirectory()) {
        nuxt.hook("components:dirs", (dirs) => {
          dirs.unshift({ path, pathPrefix: false, prefix: "" });
        });
      }
    }
    nuxt.options.routeRules ||= {};
    nuxt.options.routeRules[`/__nuxt_content/**`] = { robots: false };
    manifest.collections.forEach((collection) => {
      if (!collection.private) {
        nuxt.options.routeRules[`/__nuxt_content/${collection.name}/sql_dump.txt`] = { prerender: true };
      }
    });
    const preset = findPreset(nuxt);
    await preset?.setup?.(options, nuxt);
    options.database ||= { type: "sqlite", filename: "./contents.sqlite" };
    await refineDatabaseConfig(options._localDatabase, { rootDir: nuxt.options.rootDir, updateSqliteFileName: true });
    await refineDatabaseConfig(options.database, { rootDir: nuxt.options.rootDir });
    nuxt.options.runtimeConfig.public.content = {
      wsUrl: ""
    };
    nuxt.options.runtimeConfig.content = {
      databaseVersion,
      version,
      database: options.database,
      localDatabase: options._localDatabase,
      integrityCheck: true
    };
    nuxt.hook("nitro:config", async (config) => {
      const preset2 = findPreset(nuxt);
      await preset2.setupNitro(config, { manifest, resolver, moduleOptions: options });
      const resolveOptions = { resolver, sqliteConnector: options.experimental?.sqliteConnector || (options.experimental?.nativeSqlite ? "native" : void 0) };
      config.alias ||= {};
      config.alias["#content/adapter"] = await resolveDatabaseAdapter(config.runtimeConfig.content.database?.type || options.database.type, resolveOptions);
      config.alias["#content/local-adapter"] = await resolveDatabaseAdapter(options._localDatabase.type || "sqlite", resolveOptions);
      config.handlers ||= [];
      config.handlers.push({
        route: "/__nuxt_content/:collection/query",
        handler: resolver.resolve("./runtime/api/query.post")
      });
      if (nuxt.options.dev) {
        addPlugin({ src: resolver.resolve("./runtime/plugins/websocket.dev"), mode: "client" });
        await watchComponents(nuxt);
        const socket = await startSocketServer(nuxt, options, manifest);
        await watchContents(nuxt, options, manifest, socket);
      }
    });
    if (hasNuxtModule("nuxt-llms")) {
      installModule(resolver.resolve("./features/llms"));
    }
    await installMDCModule(options, nuxt);
    if (nuxt.options._prepare) {
      return;
    }
    nuxt.hook("modules:done", async () => {
      const fest = await processCollectionItems(nuxt, manifest.collections, options);
      manifest.checksumStructure = fest.checksumStructure;
      manifest.checksum = fest.checksum;
      manifest.dump = fest.dump;
      manifest.components = fest.components;
      await updateTemplates({
        filter: (template) => [
          moduleTemplates.fullRawDump,
          moduleTemplates.fullCompressedDump,
          moduleTemplates.manifest,
          moduleTemplates.components
        ].includes(template.filename)
      });
      if (shouldEnablePreview(nuxt, options)) {
        await setupPreview(options, nuxt, resolver, manifest);
      }
    });
  }
});
async function processCollectionItems(nuxt, collections, options) {
  const collectionDump = {};
  const collectionChecksum = {};
  const collectionChecksumStructure = {};
  const db = await getLocalDatabase(options._localDatabase, {
    sqliteConnector: options.experimental?.sqliteConnector || (options.experimental?.nativeSqlite ? "native" : void 0)
  });
  const databaseContents = await db.fetchDevelopmentCache();
  const configHash = hash({
    mdcHighlight: nuxt.options.mdc?.highlight,
    contentBuild: options.build?.markdown
  });
  const infoCollection = collections.find((c) => c.name === "info");
  const startTime = performance.now();
  let filesCount = 0;
  let cachedFilesCount = 0;
  let parsedFilesCount = 0;
  const usedComponents = [];
  db.dropContentTables();
  for await (const collection of collections) {
    if (collection.name === "info") {
      continue;
    }
    const collectionHash = hash(collection);
    const collectionQueries = generateCollectionTableDefinition(collection, { drop: true }).split("\n").map((q) => `${q} -- structure`);
    if (!collection.source) {
      continue;
    }
    const parse = await createParser(collection, nuxt);
    const structureVersion = collectionChecksumStructure[collection.name] = hash(collectionQueries);
    for await (const source of collection.source) {
      if (source.prepare) {
        const rootDir = collection.__rootDir || nuxt.options.rootDir;
        await source.prepare({ rootDir });
      }
      const { fixed } = parseSourceBase(source);
      const cwd = source.cwd;
      const _keys = await source.getKeys?.() || [];
      filesCount += _keys.length;
      const list = [];
      for await (const chunk of chunks(_keys, 25)) {
        await Promise.all(chunk.map(async (key) => {
          const keyInCollection = join(collection.name, source?.prefix || "", key);
          const fullPath = join(cwd, fixed, key);
          const cache = databaseContents[keyInCollection];
          try {
            const content = await source.getItem?.(key) || "";
            const checksum = getContentChecksum(configHash + collectionHash + content);
            let parsedContent;
            if (cache && cache.checksum === checksum) {
              cachedFilesCount += 1;
              parsedContent = JSON.parse(cache.value);
            } else {
              parsedFilesCount += 1;
              parsedContent = await parse({
                id: keyInCollection,
                body: content,
                path: fullPath
              });
              if (parsedContent) {
                db.insertDevelopmentCache(keyInCollection, JSON.stringify(parsedContent), checksum);
              }
            }
            if (parsedContent?.__metadata?.components) {
              usedComponents.push(...parsedContent.__metadata.components);
            }
            const { queries, hash: hash2 } = generateCollectionInsert(collection, parsedContent);
            list.push([key, queries, hash2]);
          } catch (e) {
            logger.warn(`"${keyInCollection}" is ignored because parsing is failed. Error: ${e instanceof Error ? e.message : "Unknown error"}`);
          }
        }));
      }
      list.sort((a, b) => String(a[0]).localeCompare(String(b[0])));
      collectionQueries.push(...list.flatMap(([, sql, hash2]) => sql.map((q) => `${q} -- ${hash2}`)));
    }
    const version2 = collectionChecksum[collection.name] = `${databaseVersion}--${hash(collectionQueries)}`;
    collectionDump[collection.name] = [
      // we have to start the series of queries
      // by telling everyone that we are setting up the collection so no
      // other request start doing the same work and fail
      // so we create a new entry in the info table saying that it is not ready yet
      // NOTE: all queries having the structure comment at the end, will be ignored at init if no
      // structure changes are detected in the structureVersion
      `${generateCollectionTableDefinition(infoCollection, { drop: false })} -- structure`,
      ...generateCollectionInsert(infoCollection, { id: `checksum_${collection.name}`, version: version2, structureVersion, ready: false }).queries.map((row) => `${row} -- meta`),
      // Insert queries for the collection
      ...collectionQueries,
      // and finally when we are finished, we update the info table to say that the init is done
      `UPDATE ${infoCollection.tableName} SET ready = true WHERE id = 'checksum_${collection.name}'; -- meta`
    ];
  }
  const sqlDumpList = Object.values(collectionDump).flatMap((a) => a);
  db.exec(`DROP TABLE IF EXISTS ${infoCollection.tableName}`);
  for (const sql of sqlDumpList) {
    db.exec(sql);
  }
  const tags = sqlDumpList.flatMap((sql) => sql.match(/(?<=(^|,|\[)\[")[^"]+(?=")/g) || []);
  const uniqueTags = [
    ...Object.values(options.renderer.alias || {}),
    ...new Set(tags),
    ...new Set(usedComponents)
  ].map((tag) => getMappedTag(tag, options?.renderer?.alias)).filter((tag) => !htmlTags.includes(kebabCase(tag))).map((tag) => pascalCase(tag));
  const endTime = performance.now();
  logger.success(`Processed ${collections.length} collections and ${filesCount} files in ${(endTime - startTime).toFixed(2)}ms (${cachedFilesCount} cached, ${parsedFilesCount} parsed)`);
  return {
    checksumStructure: collectionChecksumStructure,
    checksum: collectionChecksum,
    dump: collectionDump,
    components: uniqueTags
  };
}
const proseTags = ["h1", "h2", "h3", "h4", "h5", "h6", "p", "a", "strong", "em", "s", "code", "span", "blockquote", "pre", "hr", "img", "ul", "ol", "li", "table", "thead", "tbody", "tr", "th", "td"];
function getMappedTag(tag, additionalTags = {}) {
  if (proseTags.includes(tag)) {
    return `prose-${tag}`;
  }
  return additionalTags[tag] || tag;
}

export { module as default, defineCollection, defineCollectionSource, defineContentConfig, defineTransformer, metaStandardSchema, pageStandardSchema, property, z };

import { ParsedRobotsTxt, RobotsGroupResolved, RobotsGroupInput, PatternMapValue, BotDetectionContext } from '../dist/runtime/types.js';
export * from '../dist/runtime/types.js';

/**
 * Bot Detection Constants
 *
 * This file contains all the bot patterns and categories for bot detection.
 * Separated from util.ts for better organization and easier maintenance.
 */
interface BotPattern {
    pattern: string;
    name: string;
    secondaryPatterns?: string[];
}
type BotCategory = 'search-engine' | 'social' | 'seo' | 'ai' | 'generic' | 'automation' | 'http-tool' | 'security-scanner' | 'scraping';
type BotKind = 'awesomium' | 'cef' | 'cefsharp' | 'coachjs' | 'electron' | 'fminer' | 'geb' | 'nightmarejs' | 'phantomas' | 'phantomjs' | 'rhino' | 'selenium' | 'sequentum' | 'slimerjs' | 'webdriverio' | 'webdriver' | 'headless_chrome' | 'unknown';
type BotName = 'googlebot' | 'bingbot' | 'yandexbot' | 'baiduspider' | 'duckduckbot' | 'yahoo' | 'twitter' | 'facebook' | 'linkedin' | 'pinterest' | 'discord' | 'majestic12' | 'ahrefs' | 'semrush' | 'screaming-frog' | 'moz' | 'anthropic' | 'claude' | 'gpt' | 'google-news' | 'cohere' | 'commoncrawl' | 'perplexity' | 'requests' | 'wget' | 'curl' | 'zgrab' | 'masscan' | 'nmap' | 'nikto' | 'wpscan' | 'scrapy' | 'phantomjs' | 'headless-browser' | 'playwright' | 'selenium' | 'puppeteer' | 'generic-bot' | 'generic-spider' | 'generic-crawler' | 'generic-scraper' | BotKind | (string & Record<never, never>);
interface BotCategoryDefinition {
    type: BotCategory;
    bots: BotPattern[];
    trusted: boolean;
}
/**
 * Maps BotD fingerprinting kinds to appropriate bot categories
 */
declare function mapBotKindToCategory(botKind: BotKind): BotCategory;
declare const KNOWN_SEARCH_BOTS: BotPattern[];
declare const SOCIAL_BOTS: BotPattern[];
declare const SEO_BOTS: BotPattern[];
declare const AI_BOTS: BotPattern[];
declare const HTTP_TOOL_BOTS: BotPattern[];
declare const SECURITY_SCANNING_BOTS: BotPattern[];
declare const SCRAPING_BOTS: BotPattern[];
declare const AUTOMATION_BOTS: BotPattern[];
declare const GENERIC_BOTS: BotPattern[];
declare const BOT_MAP: BotCategoryDefinition[];

/**
 * A list of bot user agents that may not be helpful for a site.
 *
 * @credits  yoast.com/robots.txt
 */
declare const NonHelpfulBots: string[];
declare const AiBots: string[];

/**
 * We're going to read the robots.txt and extract any disallow or sitemaps rules from it.
 *
 * We're going to use the Google specification, the keys should be checked:
 *
 * - user-agent: identifies which crawler the rules apply to.
 * - allow: a URL path that may be crawled.
 * - disallow: a URL path that may not be crawled.
 * - sitemap: the complete URL of a sitemap.
 * - host: the host name of the site, this is optional non-standard directive.
 * - content-usage / content-signal: AI content usage preferences (IETF spec / Cloudflare implementation).
 *
 * @see https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt
 * @see https://github.com/google/robotstxt/blob/86d5836ba2d5a0b6b938ab49501be0e09d9c276c/robots.cc#L714C1-L720C2
 */
declare function parseRobotsTxt(s: string): ParsedRobotsTxt;
declare function matchPathToRule(path: string, _rules: Required<RobotsGroupResolved>['_rules']): Required<RobotsGroupResolved>['_rules'][number] | null;
declare function validateRobots(robotsTxt: ParsedRobotsTxt): ParsedRobotsTxt;
declare function asArray(v: any): any[];
declare function normalizeGroup(group: RobotsGroupInput | RobotsGroupResolved): RobotsGroupResolved;
declare function generateRobotsTxt({ groups, sitemaps }: {
    groups: RobotsGroupResolved[];
    sitemaps: string[];
}): string;
declare function mergeOnKey<T extends Record<string, any>, K extends keyof T>(arr: T[], key: K): T[];
declare function isInternalRoute(_path: string): boolean;

declare function createPatternMap(): Map<string, PatternMapValue>;
/**
 * Detects bots based on HTTP headers analysis
 * @param headers - HTTP headers object (similar to h3's getHeaders result)
 * @param patternMap - Optional precomputed pattern map for performance optimization
 * @returns Bot detection result with type, name, and trust level
 */
declare function isBotFromHeaders(headers: Record<string, string | string[] | undefined>, patternMap?: Map<string, PatternMapValue>): {
    isBot: boolean;
    data?: {
        botName: BotName;
        botCategory: BotCategory;
        trusted: boolean;
    };
};
/**
 * Pure bot detection function using headers
 */
declare function getBotDetection(headers: Record<string, string | string[] | undefined>, patternMap?: Map<string, PatternMapValue>): BotDetectionContext;
/**
 * Check if headers indicate a bot
 */
declare function isBot(headers: Record<string, string | string[] | undefined>, patternMap?: Map<string, PatternMapValue>): boolean;
/**
 * Get bot information if detected
 */
declare function getBotInfo(headers: Record<string, string | string[] | undefined>, patternMap?: Map<string, PatternMapValue>): {
    name: BotName | undefined;
    category: BotCategory | undefined;
    trusted: boolean | undefined;
    method: "headers" | "fingerprint" | undefined;
} | null;

export { AI_BOTS, AUTOMATION_BOTS, AiBots, BOT_MAP, GENERIC_BOTS, HTTP_TOOL_BOTS, KNOWN_SEARCH_BOTS, NonHelpfulBots, SCRAPING_BOTS, SECURITY_SCANNING_BOTS, SEO_BOTS, SOCIAL_BOTS, asArray, createPatternMap, generateRobotsTxt, getBotDetection, getBotInfo, isBot, isBotFromHeaders, isInternalRoute, mapBotKindToCategory, matchPathToRule, mergeOnKey, normalizeGroup, parseRobotsTxt, validateRobots };
export type { BotCategory, BotCategoryDefinition, BotKind, BotName, BotPattern };
